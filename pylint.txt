************* Module Music2Emotion
__init__.py:1:0: C0103: Module name "Music2Emotion" doesn't conform to snake_case naming style (invalid-name)
************* Module Music2Emotion.data_loader
data_loader.py:37:0: C0303: Trailing whitespace (trailing-whitespace)
data_loader.py:49:0: C0301: Line too long (107/100) (line-too-long)
data_loader.py:50:0: C0301: Line too long (110/100) (line-too-long)
data_loader.py:51:0: C0301: Line too long (105/100) (line-too-long)
data_loader.py:60:84: C0303: Trailing whitespace (trailing-whitespace)
data_loader.py:61:96: C0303: Trailing whitespace (trailing-whitespace)
data_loader.py:66:84: C0303: Trailing whitespace (trailing-whitespace)
data_loader.py:67:97: C0303: Trailing whitespace (trailing-whitespace)
data_loader.py:72:84: C0303: Trailing whitespace (trailing-whitespace)
data_loader.py:73:97: C0303: Trailing whitespace (trailing-whitespace)
data_loader.py:78:0: C0305: Trailing newlines (trailing-newlines)
data_loader.py:1:0: C0114: Missing module docstring (missing-module-docstring)
data_loader.py:10:0: E0401: Unable to import 'music2latent' (import-error)
data_loader.py:15:0: E0401: Unable to import 'dataset_loaders.jamendo' (import-error)
data_loader.py:16:0: E0401: Unable to import 'dataset_loaders.pmemo' (import-error)
data_loader.py:17:0: E0401: Unable to import 'dataset_loaders.deam' (import-error)
data_loader.py:18:0: E0401: Unable to import 'dataset_loaders.emomusic' (import-error)
data_loader.py:29:0: C0115: Missing class docstring (missing-class-docstring)
data_loader.py:3:0: C0411: standard import "pickle" should be placed before third party import "numpy" (wrong-import-order)
data_loader.py:8:0: C0411: standard import "csv" should be placed before third party imports "numpy", "torch.utils.data", "torchaudio.transforms", "torchaudio", "torch" (wrong-import-order)
data_loader.py:11:0: C0411: standard import "json" should be placed before third party imports "numpy", "torch.utils.data", "torchaudio.transforms" (...) "torch", "pytorch_lightning", "music2latent.EncoderDecoder" (wrong-import-order)
data_loader.py:12:0: C0411: standard import "math" should be placed before third party imports "numpy", "torch.utils.data", "torchaudio.transforms" (...) "torch", "pytorch_lightning", "music2latent.EncoderDecoder" (wrong-import-order)
data_loader.py:20:0: C0411: third party import "omegaconf.DictConfig" should be placed before first party imports "dataset_loaders.jamendo.JamendoDataset", "dataset_loaders.pmemo.PMEmoDataset", "dataset_loaders.deam.DEAMDataset", "dataset_loaders.emomusic.EmoMusicDataset"  (wrong-import-order)
data_loader.py:1:0: W0611: Unused import os (unused-import)
data_loader.py:2:0: W0611: Unused numpy imported as np (unused-import)
data_loader.py:3:0: W0611: Unused import pickle (unused-import)
data_loader.py:5:0: W0611: Unused torchaudio.transforms imported as T (unused-import)
data_loader.py:6:0: W0611: Unused import torchaudio (unused-import)
data_loader.py:7:0: W0611: Unused import torch (unused-import)
data_loader.py:8:0: W0611: Unused import csv (unused-import)
data_loader.py:10:0: W0611: Unused EncoderDecoder imported from music2latent (unused-import)
data_loader.py:11:0: W0611: Unused import json (unused-import)
data_loader.py:12:0: W0611: Unused import math (unused-import)
data_loader.py:13:0: W0611: Unused StandardScaler imported from sklearn.preprocessing (unused-import)
************* Module Music2Emotion.music2emo
music2emo.py:11:0: C0301: Line too long (130/100) (line-too-long)
music2emo.py:95:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:97:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:100:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:105:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:129:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:131:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:153:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:155:26: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:165:0: C0301: Line too long (119/100) (line-too-long)
music2emo.py:170:0: C0301: Line too long (123/100) (line-too-long)
music2emo.py:200:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:205:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:211:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:222:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:228:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:229:20: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:233:0: C0301: Line too long (109/100) (line-too-long)
music2emo.py:236:0: C0301: Line too long (106/100) (line-too-long)
music2emo.py:261:0: C0301: Line too long (110/100) (line-too-long)
music2emo.py:264:0: C0301: Line too long (108/100) (line-too-long)
music2emo.py:277:0: C0301: Line too long (104/100) (line-too-long)
music2emo.py:280:0: C0325: Unnecessary parens after 'assert' keyword (superfluous-parens)
music2emo.py:281:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:283:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:299:0: C0301: Line too long (112/100) (line-too-long)
music2emo.py:308:0: C0301: Line too long (120/100) (line-too-long)
music2emo.py:313:0: C0301: Line too long (133/100) (line-too-long)
music2emo.py:316:0: C0301: Line too long (125/100) (line-too-long)
music2emo.py:340:45: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:362:0: C0301: Line too long (104/100) (line-too-long)
music2emo.py:364:0: C0301: Line too long (108/100) (line-too-long)
music2emo.py:366:95: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:367:0: C0301: Line too long (112/100) (line-too-long)
music2emo.py:368:0: W0311: Bad indentation. Found 13 spaces, expected 12 (bad-indentation)
music2emo.py:369:0: C0301: Line too long (112/100) (line-too-long)
music2emo.py:370:0: W0311: Bad indentation. Found 13 spaces, expected 12 (bad-indentation)
music2emo.py:381:57: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:386:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:393:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:399:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:431:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:433:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:437:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:447:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:454:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:469:0: C0301: Line too long (104/100) (line-too-long)
music2emo.py:472:0: C0301: Line too long (104/100) (line-too-long)
music2emo.py:476:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:477:0: C0301: Line too long (109/100) (line-too-long)
music2emo.py:483:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:488:0: C0303: Trailing whitespace (trailing-whitespace)
music2emo.py:499:0: C0301: Line too long (106/100) (line-too-long)
music2emo.py:1:0: C0114: Missing module docstring (missing-module-docstring)
music2emo.py:8:0: W0401: Wildcard import Music2Emotion.utils.transformer_modules (wildcard-import)
music2emo.py:15:0: W0404: Reimport 'os' (imported line 1) (reimported)
music2emo.py:31:0: W0404: Reimport 'warnings' (imported line 13) (reimported)
music2emo.py:75:0: C0103: Constant name "segment_duration" doesn't conform to UPPER_CASE naming style (invalid-name)
music2emo.py:76:0: C0103: Constant name "resample_rate" doesn't conform to UPPER_CASE naming style (invalid-name)
music2emo.py:77:0: C0103: Constant name "is_split" doesn't conform to UPPER_CASE naming style (invalid-name)
music2emo.py:79:0: C0116: Missing function or method docstring (missing-function-docstring)
music2emo.py:79:0: R0914: Too many local variables (19/15) (too-many-locals)
music2emo.py:80:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
music2emo.py:93:8: R1715: Consider using dict.get for getting values from a dict if a key is present or a default if not (consider-using-get)
music2emo.py:130:62: E0606: Possibly using variable 'newchordnorm' before assignment (possibly-used-before-assignment)
music2emo.py:79:0: R0912: Too many branches (13/12) (too-many-branches)
music2emo.py:84:8: W0612: Unused variable 'new_key' (unused-variable)
music2emo.py:134:0: C0116: Missing function or method docstring (missing-function-docstring)
music2emo.py:137:0: C0116: Missing function or method docstring (missing-function-docstring)
music2emo.py:143:0: C0116: Missing function or method docstring (missing-function-docstring)
music2emo.py:162:0: C0115: Missing class docstring (missing-class-docstring)
music2emo.py:195:4: C0116: Missing function or method docstring (missing-function-docstring)
music2emo.py:195:4: R0914: Too many local variables (110/15) (too-many-locals)
music2emo.py:235:55: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
music2emo.py:262:8: E1101: Instance of 'HParams' has no 'feature' member (no-member)
music2emo.py:263:8: E1101: Instance of 'HParams' has no 'model' member (no-member)
music2emo.py:266:33: E1101: Instance of 'HParams' has no 'model' member (no-member)
music2emo.py:278:8: W0702: No exception type(s) specified (bare-except)
music2emo.py:279:24: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
music2emo.py:282:20: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
music2emo.py:285:29: E0606: Possibly using variable 'mean' before assignment (possibly-used-before-assignment)
music2emo.py:285:37: E0606: Possibly using variable 'std' before assignment (possibly-used-before-assignment)
music2emo.py:287:21: E1101: Instance of 'HParams' has no 'model' member (no-member)
music2emo.py:308:28: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
music2emo.py:313:41: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
music2emo.py:317:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
music2emo.py:324:31: R1734: Consider using [] instead of list() (use-list-literal)
music2emo.py:324:39: R1734: Consider using [] instead of list() (use-list-literal)
music2emo.py:324:47: R1734: Consider using [] instead of list() (use-list-literal)
music2emo.py:337:24: W0106: Expression "(starts.append(start_time), ends.append(interval[0]), pitchs.append(p + 48))" is assigned to nothing (expression-not-assigned)
music2emo.py:342:24: W0106: Expression "(starts.append(start_time), ends.append(interval[1]), pitchs.append(p + 48))" is assigned to nothing (expression-not-assigned)
music2emo.py:362:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
music2emo.py:364:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
music2emo.py:367:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
music2emo.py:368:13: C0103: Variable name "chordRootDic" doesn't conform to snake_case naming style (invalid-name)
music2emo.py:369:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
music2emo.py:370:13: C0103: Variable name "chordAttrDic" doesn't conform to snake_case naming style (invalid-name)
music2emo.py:374:15: W0718: Catching too general exception Exception (broad-exception-caught)
music2emo.py:395:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
music2emo.py:403:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
music2emo.py:416:16: C0103: Variable name "chordRootID" doesn't conform to snake_case naming style (invalid-name)
music2emo.py:418:20: C0103: Variable name "chordAttrID" doesn't conform to snake_case naming style (invalid-name)
music2emo.py:420:20: C0103: Variable name "chordAttrID" doesn't conform to snake_case naming style (invalid-name)
music2emo.py:422:16: C0103: Variable name "chordRootID" doesn't conform to snake_case naming style (invalid-name)
music2emo.py:423:16: C0103: Variable name "chordAttrID" doesn't conform to snake_case naming style (invalid-name)
music2emo.py:424:32: E0606: Possibly using variable 'chordRootID' before assignment (possibly-used-before-assignment)
music2emo.py:425:32: E0606: Possibly using variable 'chordAttrID' before assignment (possibly-used-before-assignment)
music2emo.py:475:8: W0127: Assigning the same variable 'threshold' to itself (self-assigning-variable)
music2emo.py:195:4: R0912: Too many branches (42/12) (too-many-branches)
music2emo.py:195:4: R0915: Too many statements (202/50) (too-many-statements)
music2emo.py:199:8: W0612: Unused variable 'current_dir' (unused-variable)
music2emo.py:238:8: W0612: Unused variable 'embeddings' (unused-variable)
music2emo.py:275:8: W0612: Unused variable 'audio_id' (unused-variable)
music2emo.py:277:41: W0612: Unused variable 'song_length_second' (unused-variable)
music2emo.py:359:8: W0612: Unused variable 'idx_to_tonic' (unused-variable)
music2emo.py:360:8: W0612: Unused variable 'idx_to_mode' (unused-variable)
music2emo.py:374:8: W0612: Unused variable 'e' (unused-variable)
music2emo.py:162:0: R0903: Too few public methods (1/2) (too-few-public-methods)
music2emo.py:4:0: C0411: standard import "sys" should be placed before third party imports "mir_eval", "pretty_midi" (wrong-import-order)
music2emo.py:12:0: C0411: standard import "argparse" should be placed before third party imports "mir_eval", "pretty_midi" and first party imports "Music2Emotion.utils.logger", "Music2Emotion.utils.btc_model.BTC_model", "Music2Emotion.utils.transformer_modules.*", "Music2Emotion.utils.transformer_modules._gen_timing_signal", "Music2Emotion.utils.hparams.HParams", "Music2Emotion.utils.mir_eval_modules.audio_file_to_features"  (wrong-import-order)
music2emo.py:13:0: C0411: standard import "warnings" should be placed before third party imports "mir_eval", "pretty_midi" and first party imports "Music2Emotion.utils.logger", "Music2Emotion.utils.btc_model.BTC_model", "Music2Emotion.utils.transformer_modules.*", "Music2Emotion.utils.transformer_modules._gen_timing_signal", "Music2Emotion.utils.hparams.HParams", "Music2Emotion.utils.mir_eval_modules.audio_file_to_features"  (wrong-import-order)
music2emo.py:14:0: C0411: third party import "music21.converter" should be placed before first party imports "Music2Emotion.utils.logger", "Music2Emotion.utils.btc_model.BTC_model", "Music2Emotion.utils.transformer_modules.*", "Music2Emotion.utils.transformer_modules._gen_timing_signal", "Music2Emotion.utils.hparams.HParams", "Music2Emotion.utils.mir_eval_modules.audio_file_to_features"  (wrong-import-order)
music2emo.py:15:0: C0411: standard import "os" should be placed before third party imports "mir_eval", "pretty_midi", "music21.converter" and first party imports "Music2Emotion.utils.logger", "Music2Emotion.utils.btc_model.BTC_model", "Music2Emotion.utils.transformer_modules.*", "Music2Emotion.utils.transformer_modules._gen_timing_signal", "Music2Emotion.utils.hparams.HParams", "Music2Emotion.utils.mir_eval_modules.audio_file_to_features"  (wrong-import-order)
music2emo.py:16:0: C0411: third party import "tqdm.tqdm" should be placed before first party imports "Music2Emotion.utils.logger", "Music2Emotion.utils.btc_model.BTC_model", "Music2Emotion.utils.transformer_modules.*", "Music2Emotion.utils.transformer_modules._gen_timing_signal", "Music2Emotion.utils.hparams.HParams", "Music2Emotion.utils.mir_eval_modules.audio_file_to_features"  (wrong-import-order)
music2emo.py:17:0: C0411: standard import "json" should be placed before third party imports "mir_eval", "pretty_midi", "music21.converter", "tqdm.tqdm" and first party imports "Music2Emotion.utils.logger", "Music2Emotion.utils.btc_model.BTC_model", "Music2Emotion.utils.transformer_modules.*", "Music2Emotion.utils.transformer_modules._gen_timing_signal", "Music2Emotion.utils.hparams.HParams", "Music2Emotion.utils.mir_eval_modules.audio_file_to_features"  (wrong-import-order)
music2emo.py:18:0: C0411: third party import "torch" should be placed before first party imports "Music2Emotion.utils.logger", "Music2Emotion.utils.btc_model.BTC_model", "Music2Emotion.utils.transformer_modules.*", "Music2Emotion.utils.transformer_modules._gen_timing_signal", "Music2Emotion.utils.hparams.HParams", "Music2Emotion.utils.mir_eval_modules.audio_file_to_features"  (wrong-import-order)
music2emo.py:19:0: C0411: third party import "torchaudio" should be placed before first party imports "Music2Emotion.utils.logger", "Music2Emotion.utils.btc_model.BTC_model", "Music2Emotion.utils.transformer_modules.*", "Music2Emotion.utils.transformer_modules._gen_timing_signal", "Music2Emotion.utils.hparams.HParams", "Music2Emotion.utils.mir_eval_modules.audio_file_to_features"  (wrong-import-order)
music2emo.py:20:0: C0411: third party import "torchaudio.transforms" should be placed before first party imports "Music2Emotion.utils.logger", "Music2Emotion.utils.btc_model.BTC_model", "Music2Emotion.utils.transformer_modules.*", "Music2Emotion.utils.transformer_modules._gen_timing_signal", "Music2Emotion.utils.hparams.HParams", "Music2Emotion.utils.mir_eval_modules.audio_file_to_features"  (wrong-import-order)
music2emo.py:21:0: C0411: third party import "numpy" should be placed before first party imports "Music2Emotion.utils.logger", "Music2Emotion.utils.btc_model.BTC_model", "Music2Emotion.utils.transformer_modules.*", "Music2Emotion.utils.transformer_modules._gen_timing_signal", "Music2Emotion.utils.hparams.HParams", "Music2Emotion.utils.mir_eval_modules.audio_file_to_features"  (wrong-import-order)
music2emo.py:22:0: C0411: third party import "omegaconf.DictConfig" should be placed before first party imports "Music2Emotion.utils.logger", "Music2Emotion.utils.btc_model.BTC_model", "Music2Emotion.utils.transformer_modules.*", "Music2Emotion.utils.transformer_modules._gen_timing_signal", "Music2Emotion.utils.hparams.HParams", "Music2Emotion.utils.mir_eval_modules.audio_file_to_features"  (wrong-import-order)
music2emo.py:23:0: C0411: third party import "hydra" should be placed before first party imports "Music2Emotion.utils.logger", "Music2Emotion.utils.btc_model.BTC_model", "Music2Emotion.utils.transformer_modules.*", "Music2Emotion.utils.transformer_modules._gen_timing_signal", "Music2Emotion.utils.hparams.HParams", "Music2Emotion.utils.mir_eval_modules.audio_file_to_features"  (wrong-import-order)
music2emo.py:24:0: C0411: third party import "hydra.utils.to_absolute_path" should be placed before first party imports "Music2Emotion.utils.logger", "Music2Emotion.utils.btc_model.BTC_model", "Music2Emotion.utils.transformer_modules.*", "Music2Emotion.utils.transformer_modules._gen_timing_signal", "Music2Emotion.utils.hparams.HParams", "Music2Emotion.utils.mir_eval_modules.audio_file_to_features"  (wrong-import-order)
music2emo.py:25:0: C0411: third party import "transformers.Wav2Vec2FeatureExtractor" should be placed before first party imports "Music2Emotion.utils.logger", "Music2Emotion.utils.btc_model.BTC_model", "Music2Emotion.utils.transformer_modules.*", "Music2Emotion.utils.transformer_modules._gen_timing_signal", "Music2Emotion.utils.hparams.HParams", "Music2Emotion.utils.mir_eval_modules.audio_file_to_features"  (wrong-import-order)
music2emo.py:28:0: C0411: standard import "pathlib.Path" should be placed before third party imports "mir_eval", "pretty_midi", "music21.converter" (...) "hydra", "hydra.utils.to_absolute_path", "transformers.Wav2Vec2FeatureExtractor" and first party imports "Music2Emotion.utils.logger", "Music2Emotion.utils.btc_model.BTC_model", "Music2Emotion.utils.transformer_modules.*" (...) "Music2Emotion.utils.mir_eval_modules.audio_file_to_features", "Music2Emotion.utils.mert.FeatureExtractorMERT", "Music2Emotion.model.linear_mt_attn_ck.FeedforwardModelMTAttnCK"  (wrong-import-order)
music2emo.py:30:0: C0411: standard import "shutil" should be placed before third party imports "mir_eval", "pretty_midi", "music21.converter" (...) "hydra", "hydra.utils.to_absolute_path", "transformers.Wav2Vec2FeatureExtractor" and first party imports "Music2Emotion.utils.logger", "Music2Emotion.utils.btc_model.BTC_model", "Music2Emotion.utils.transformer_modules.*" (...) "Music2Emotion.utils.mir_eval_modules.audio_file_to_features", "Music2Emotion.utils.mert.FeatureExtractorMERT", "Music2Emotion.model.linear_mt_attn_ck.FeedforwardModelMTAttnCK"  (wrong-import-order)
music2emo.py:31:0: C0411: standard import "warnings" should be placed before third party imports "mir_eval", "pretty_midi", "music21.converter" (...) "hydra", "hydra.utils.to_absolute_path", "transformers.Wav2Vec2FeatureExtractor" and first party imports "Music2Emotion.utils.logger", "Music2Emotion.utils.btc_model.BTC_model", "Music2Emotion.utils.transformer_modules.*" (...) "Music2Emotion.utils.mir_eval_modules.audio_file_to_features", "Music2Emotion.utils.mert.FeatureExtractorMERT", "Music2Emotion.model.linear_mt_attn_ck.FeedforwardModelMTAttnCK"  (wrong-import-order)
music2emo.py:33:0: C0411: standard import "logging" should be placed before third party imports "mir_eval", "pretty_midi", "music21.converter" (...) "hydra", "hydra.utils.to_absolute_path", "transformers.Wav2Vec2FeatureExtractor" and first party imports "Music2Emotion.utils.logger", "Music2Emotion.utils.btc_model.BTC_model", "Music2Emotion.utils.transformer_modules.*" (...) "Music2Emotion.utils.mir_eval_modules.audio_file_to_features", "Music2Emotion.utils.mert.FeatureExtractorMERT", "Music2Emotion.model.linear_mt_attn_ck.FeedforwardModelMTAttnCK"  (wrong-import-order)
music2emo.py:15:0: C0412: Imports from package os are not grouped (ungrouped-imports)
music2emo.py:31:0: C0412: Imports from package warnings are not grouped (ungrouped-imports)
music2emo.py:26:0: C0412: Imports from package Music2Emotion are not grouped (ungrouped-imports)
music2emo.py:4:0: W0611: Unused import sys (unused-import)
music2emo.py:9:0: W0611: Unused _gen_timing_signal imported from Music2Emotion.utils.transformer_modules (unused-import)
music2emo.py:9:0: W0611: Unused _gen_bias_mask imported from Music2Emotion.utils.transformer_modules (unused-import)
music2emo.py:11:0: W0611: Unused idx2chord imported from Music2Emotion.utils.mir_eval_modules (unused-import)
music2emo.py:11:0: W0611: Unused get_audio_paths imported from Music2Emotion.utils.mir_eval_modules (unused-import)
music2emo.py:11:0: W0611: Unused get_lab_paths imported from Music2Emotion.utils.mir_eval_modules (unused-import)
music2emo.py:12:0: W0611: Unused import argparse (unused-import)
music2emo.py:16:0: W0611: Unused tqdm imported from tqdm (unused-import)
music2emo.py:22:0: W0611: Unused DictConfig imported from omegaconf (unused-import)
music2emo.py:23:0: W0611: Unused import hydra (unused-import)
music2emo.py:24:0: W0611: Unused to_absolute_path imported from hydra.utils (unused-import)
music2emo.py:25:0: W0611: Unused Wav2Vec2FeatureExtractor imported from transformers (unused-import)
music2emo.py:25:0: W0611: Unused AutoModel imported from transformers (unused-import)
music2emo.py:8:0: W0614: Unused import(s) nn, F, math, LayerNorm, OutputLayer, SoftmaxOutputLayer, MultiHeadAttention, Conv and PositionwiseFeedForward from wildcard import of Music2Emotion.utils.transformer_modules (unused-wildcard-import)
************* Module Music2Emotion.test
test.py:39:0: C0303: Trailing whitespace (trailing-whitespace)
test.py:42:0: C0303: Trailing whitespace (trailing-whitespace)
test.py:44:0: C0301: Line too long (108/100) (line-too-long)
test.py:46:0: C0301: Line too long (106/100) (line-too-long)
test.py:48:0: C0301: Line too long (101/100) (line-too-long)
test.py:52:0: C0303: Trailing whitespace (trailing-whitespace)
test.py:79:0: C0303: Trailing whitespace (trailing-whitespace)
test.py:84:0: C0301: Line too long (120/100) (line-too-long)
test.py:88:53: C0303: Trailing whitespace (trailing-whitespace)
test.py:89:40: C0303: Trailing whitespace (trailing-whitespace)
test.py:93:0: C0303: Trailing whitespace (trailing-whitespace)
test.py:94:0: C0303: Trailing whitespace (trailing-whitespace)
test.py:1:0: C0114: Missing module docstring (missing-module-docstring)
test.py:7:0: C0413: Import "import pytorch_lightning as pl" should be placed at the top of the module (wrong-import-position)
test.py:8:0: C0413: Import "from pytorch_lightning.loggers import TensorBoardLogger" should be placed at the top of the module (wrong-import-position)
test.py:9:0: C0413: Import "from pytorch_lightning.callbacks import ModelCheckpoint" should be placed at the top of the module (wrong-import-position)
test.py:10:0: E0401: Unable to import 'data_loader' (import-error)
test.py:10:0: C0413: Import "from data_loader import DataModule" should be placed at the top of the module (wrong-import-position)
test.py:11:0: E0401: Unable to import 'trainer' (import-error)
test.py:11:0: C0413: Import "from trainer import MusicClassifier" should be placed at the top of the module (wrong-import-position)
test.py:12:0: C0413: Import "import yaml" should be placed at the top of the module (wrong-import-position)
test.py:13:0: C0413: Import "from omegaconf import DictConfig" should be placed at the top of the module (wrong-import-position)
test.py:14:0: C0413: Import "import hydra" should be placed at the top of the module (wrong-import-position)
test.py:15:0: C0413: Import "from hydra.utils import to_absolute_path" should be placed at the top of the module (wrong-import-position)
test.py:16:0: C0413: Import "from hydra.core.hydra_config import HydraConfig" should be placed at the top of the module (wrong-import-position)
test.py:17:0: C0413: Import "from pytorch_lightning.utilities.combined_loader import CombinedLoader" should be placed at the top of the module (wrong-import-position)
test.py:22:0: C0116: Missing function or method docstring (missing-function-docstring)
test.py:27:0: C0116: Missing function or method docstring (missing-function-docstring)
test.py:32:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
test.py:40:9: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
test.py:56:0: C0116: Missing function or method docstring (missing-function-docstring)
test.py:80:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
test.py:84:18: R1721: Unnecessary use of a comprehension, use dict(zip(config.datasets, data_module.test_dataloader())) instead. (unnecessary-comprehension)
test.py:98:4: E1120: No value for argument 'config' in function call (no-value-for-parameter)
test.py:12:0: C0411: third party import "yaml" should be placed before first party imports "data_loader.DataModule", "trainer.MusicClassifier"  (wrong-import-order)
test.py:13:0: C0411: third party import "omegaconf.DictConfig" should be placed before first party imports "data_loader.DataModule", "trainer.MusicClassifier"  (wrong-import-order)
test.py:14:0: C0411: third party import "hydra" should be placed before first party imports "data_loader.DataModule", "trainer.MusicClassifier"  (wrong-import-order)
test.py:15:0: C0411: third party import "hydra.utils.to_absolute_path" should be placed before first party imports "data_loader.DataModule", "trainer.MusicClassifier"  (wrong-import-order)
test.py:16:0: C0411: third party import "hydra.core.hydra_config.HydraConfig" should be placed before first party imports "data_loader.DataModule", "trainer.MusicClassifier"  (wrong-import-order)
test.py:17:0: C0411: third party import "pytorch_lightning.utilities.combined_loader.CombinedLoader" should be placed before first party imports "data_loader.DataModule", "trainer.MusicClassifier"  (wrong-import-order)
test.py:17:0: C0412: Imports from package pytorch_lightning are not grouped (ungrouped-imports)
test.py:9:0: W0611: Unused ModelCheckpoint imported from pytorch_lightning.callbacks (unused-import)
test.py:15:0: W0611: Unused to_absolute_path imported from hydra.utils (unused-import)
test.py:16:0: W0611: Unused HydraConfig imported from hydra.core.hydra_config (unused-import)
************* Module Music2Emotion.train
train.py:5:44: C0303: Trailing whitespace (trailing-whitespace)
train.py:47:0: C0301: Line too long (122/100) (line-too-long)
train.py:48:0: C0301: Line too long (117/100) (line-too-long)
train.py:49:0: C0303: Trailing whitespace (trailing-whitespace)
train.py:93:0: C0303: Trailing whitespace (trailing-whitespace)
train.py:1:0: C0114: Missing module docstring (missing-module-docstring)
train.py:8:0: C0413: Import "import pytorch_lightning as pl" should be placed at the top of the module (wrong-import-position)
train.py:9:0: C0413: Import "from pytorch_lightning.loggers import TensorBoardLogger" should be placed at the top of the module (wrong-import-position)
train.py:10:0: C0413: Import "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint" should be placed at the top of the module (wrong-import-position)
train.py:11:0: E0401: Unable to import 'data_loader' (import-error)
train.py:11:0: C0413: Import "from data_loader import DataModule" should be placed at the top of the module (wrong-import-position)
train.py:12:0: E0401: Unable to import 'trainer' (import-error)
train.py:12:0: C0413: Import "from trainer import MusicClassifier" should be placed at the top of the module (wrong-import-position)
train.py:13:0: C0413: Import "from omegaconf import DictConfig" should be placed at the top of the module (wrong-import-position)
train.py:14:0: C0413: Import "import hydra" should be placed at the top of the module (wrong-import-position)
train.py:15:0: C0413: Import "from hydra.utils import to_absolute_path" should be placed at the top of the module (wrong-import-position)
train.py:16:0: C0413: Import "from hydra.core.hydra_config import HydraConfig" should be placed at the top of the module (wrong-import-position)
train.py:17:0: C0413: Import "from pytorch_lightning.callbacks import EarlyStopping" should be placed at the top of the module (wrong-import-position)
train.py:18:0: C0413: Import "from pytorch_lightning.utilities.combined_loader import CombinedLoader" should be placed at the top of the module (wrong-import-position)
train.py:19:0: C0413: Import "from pytorch_lightning.strategies import DDPStrategy" should be placed at the top of the module (wrong-import-position)
train.py:22:0: C0116: Missing function or method docstring (missing-function-docstring)
train.py:29:0: C0116: Missing function or method docstring (missing-function-docstring)
train.py:29:0: R0914: Too many local variables (21/15) (too-many-locals)
train.py:47:19: R1721: Unnecessary use of a comprehension, use dict(zip(config.datasets, data_module.train_dataloader())) instead. (unnecessary-comprehension)
train.py:48:16: R1721: Unnecessary use of a comprehension, use dict(zip(config.datasets, data_module.val_dataloader())) instead. (unnecessary-comprehension)
train.py:67:11: C0121: Comparison 'config.model.kd == True' should be 'config.model.kd is True' if checking for the singleton value True, or 'config.model.kd' if testing for truthiness (singleton-comparison)
train.py:98:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
train.py:107:4: E1120: No value for argument 'config' in function call (no-value-for-parameter)
train.py:13:0: C0411: third party import "omegaconf.DictConfig" should be placed before first party imports "data_loader.DataModule", "trainer.MusicClassifier"  (wrong-import-order)
train.py:14:0: C0411: third party import "hydra" should be placed before first party imports "data_loader.DataModule", "trainer.MusicClassifier"  (wrong-import-order)
train.py:15:0: C0411: third party import "hydra.utils.to_absolute_path" should be placed before first party imports "data_loader.DataModule", "trainer.MusicClassifier"  (wrong-import-order)
train.py:16:0: C0411: third party import "hydra.core.hydra_config.HydraConfig" should be placed before first party imports "data_loader.DataModule", "trainer.MusicClassifier"  (wrong-import-order)
train.py:17:0: C0411: third party import "pytorch_lightning.callbacks.EarlyStopping" should be placed before first party imports "data_loader.DataModule", "trainer.MusicClassifier"  (wrong-import-order)
train.py:18:0: C0411: third party import "pytorch_lightning.utilities.combined_loader.CombinedLoader" should be placed before first party imports "data_loader.DataModule", "trainer.MusicClassifier"  (wrong-import-order)
train.py:19:0: C0411: third party import "pytorch_lightning.strategies.DDPStrategy" should be placed before first party imports "data_loader.DataModule", "trainer.MusicClassifier"  (wrong-import-order)
train.py:17:0: C0412: Imports from package pytorch_lightning are not grouped (ungrouped-imports)
train.py:10:0: W0611: Unused LearningRateMonitor imported from pytorch_lightning.callbacks (unused-import)
train.py:15:0: W0611: Unused to_absolute_path imported from hydra.utils (unused-import)
train.py:16:0: W0611: Unused HydraConfig imported from hydra.core.hydra_config (unused-import)
************* Module Music2Emotion.trainer
trainer.py:75:73: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:78:82: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:87:0: C0301: Line too long (115/100) (line-too-long)
trainer.py:90:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:91:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:94:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:98:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:105:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:107:0: C0301: Line too long (109/100) (line-too-long)
trainer.py:109:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:111:0: C0301: Line too long (104/100) (line-too-long)
trainer.py:112:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:115:0: C0301: Line too long (112/100) (line-too-long)
trainer.py:116:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:122:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:129:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:136:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:137:46: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:168:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:178:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:184:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:219:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:230:35: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:235:0: C0301: Line too long (119/100) (line-too-long)
trainer.py:247:49: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:256:0: C0301: Line too long (119/100) (line-too-long)
trainer.py:289:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:293:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:296:0: C0301: Line too long (160/100) (line-too-long)
trainer.py:297:0: C0301: Line too long (156/100) (line-too-long)
trainer.py:298:0: C0301: Line too long (139/100) (line-too-long)
trainer.py:301:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:311:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:314:0: C0301: Line too long (158/100) (line-too-long)
trainer.py:315:0: C0301: Line too long (154/100) (line-too-long)
trainer.py:316:0: C0301: Line too long (137/100) (line-too-long)
trainer.py:333:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:339:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:346:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:363:0: C0301: Line too long (101/100) (line-too-long)
trainer.py:364:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:375:0: C0301: Line too long (164/100) (line-too-long)
trainer.py:376:0: C0301: Line too long (160/100) (line-too-long)
trainer.py:377:0: C0301: Line too long (138/100) (line-too-long)
trainer.py:386:0: C0301: Line too long (101/100) (line-too-long)
trainer.py:431:0: C0301: Line too long (111/100) (line-too-long)
trainer.py:432:0: C0301: Line too long (111/100) (line-too-long)
trainer.py:438:0: C0301: Line too long (105/100) (line-too-long)
trainer.py:439:0: C0301: Line too long (101/100) (line-too-long)
trainer.py:440:0: C0301: Line too long (105/100) (line-too-long)
trainer.py:441:0: C0301: Line too long (101/100) (line-too-long)
trainer.py:461:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:468:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:470:0: C0301: Line too long (145/100) (line-too-long)
trainer.py:471:155: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:471:0: C0301: Line too long (155/100) (line-too-long)
trainer.py:477:0: C0303: Trailing whitespace (trailing-whitespace)
trainer.py:478:0: C0305: Trailing newlines (trailing-newlines)
trainer.py:1:0: C0114: Missing module docstring (missing-module-docstring)
trainer.py:3:0: R0402: Use 'from torch import nn' instead (consider-using-from-import)
trainer.py:13:0: E0401: Unable to import 'model.linear' (import-error)
trainer.py:14:0: E0401: Unable to import 'model.linear_small' (import-error)
trainer.py:15:0: E0401: Unable to import 'model.linear_attn_ck' (import-error)
trainer.py:16:0: E0401: Unable to import 'model.linear_mt' (import-error)
trainer.py:17:0: E0401: Unable to import 'model.linear_mt_attn_ck' (import-error)
trainer.py:23:0: W0404: Reimport 'torch' (imported line 2) (reimported)
trainer.py:26:0: E0401: Unable to import 'torch_optimizer' (import-error)
trainer.py:39:0: C0115: Missing class docstring (missing-class-docstring)
trainer.py:39:0: R0902: Too many instance attributes (30/7) (too-many-instance-attributes)
trainer.py:40:4: R0914: Too many local variables (18/15) (too-many-locals)
trainer.py:41:8: R1725: Consider using Python 3 style super() without arguments (super-with-arguments)
trainer.py:64:16: W0719: Raising too general exception: Exception (broad-exception-raised)
trainer.py:75:16: W0719: Raising too general exception: Exception (broad-exception-raised)
trainer.py:78:16: W0719: Raising too general exception: Exception (broad-exception-raised)
trainer.py:78:32: W1309: Using an f-string that does not have any interpolated variables (f-string-without-interpolation)
trainer.py:89:16: W0719: Raising too general exception: Exception (broad-exception-raised)
trainer.py:40:4: R0915: Too many statements (67/50) (too-many-statements)
trainer.py:156:4: W0221: Number of parameters was 3 in 'LightningModule.forward' and is now 3 in overriding 'MusicClassifier.forward' method (arguments-differ)
trainer.py:156:4: W0221: Variadics removed in overriding 'MusicClassifier.forward' method (arguments-differ)
trainer.py:159:12: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
trainer.py:156:4: R1710: Either all return statements in a function should return an expression, or none of them should. (inconsistent-return-statements)
trainer.py:169:4: C0116: Missing function or method docstring (missing-function-docstring)
trainer.py:174:4: C0116: Missing function or method docstring (missing-function-docstring)
trainer.py:179:4: C0116: Missing function or method docstring (missing-function-docstring)
trainer.py:220:4: C0116: Missing function or method docstring (missing-function-docstring)
trainer.py:220:4: R0913: Too many arguments (6/5) (too-many-arguments)
trainer.py:220:4: R0917: Too many positional arguments (6/5) (too-many-positional-arguments)
trainer.py:220:4: R0912: Too many branches (17/12) (too-many-branches)
trainer.py:220:4: R0915: Too many statements (57/50) (too-many-statements)
trainer.py:285:4: W0221: Number of parameters was 3 in 'LightningModule.training_step' and is now 3 in overriding 'MusicClassifier.training_step' method (arguments-differ)
trainer.py:285:4: W0221: Variadics removed in overriding 'MusicClassifier.training_step' method (arguments-differ)
trainer.py:285:35: W0613: Unused argument 'batch_idx' (unused-argument)
trainer.py:302:4: W0221: Number of parameters was 3 in 'LightningModule.validation_step' and is now 3 in overriding 'MusicClassifier.validation_step' method (arguments-differ)
trainer.py:302:4: W0221: Variadics removed in overriding 'MusicClassifier.validation_step' method (arguments-differ)
trainer.py:302:37: W0613: Unused argument 'batch_idx' (unused-argument)
trainer.py:319:4: W0221: Number of parameters was 3 in 'LightningModule.test_step' and is now 3 in overriding 'MusicClassifier.test_step' method (arguments-differ)
trainer.py:319:4: W0221: Variadics removed in overriding 'MusicClassifier.test_step' method (arguments-differ)
trainer.py:319:4: R0914: Too many local variables (18/15) (too-many-locals)
trainer.py:319:31: W0613: Unused argument 'batch_idx' (unused-argument)
trainer.py:381:4: R0914: Too many local variables (19/15) (too-many-locals)
trainer.py:392:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
trainer.py:393:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
trainer.py:396:21: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
trainer.py:428:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
trainer.py:429:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
trainer.py:430:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
trainer.py:431:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
trainer.py:432:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
trainer.py:435:25: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
trainer.py:381:4: R0915: Too many statements (52/50) (too-many-statements)
trainer.py:462:4: C0116: Missing function or method docstring (missing-function-docstring)
trainer.py:345:24: W0201: Attribute 'jamendo_results' defined outside __init__ (attribute-defined-outside-init)
trainer.py:9:0: C0411: standard import "collections.OrderedDict" should be placed before third party imports "torch", "torch.nn", "torch.nn.functional" (...) "sklearn.metrics", "transformers.AutoModelForAudioClassification", "numpy" (wrong-import-order)
trainer.py:19:0: C0411: standard import "logging" should be placed before third party imports "torch", "torch.nn", "torch.nn.functional" (...) "numpy", "torchmetrics.MeanMetric", "torchmetrics.functional" and first party imports "model.linear.FeedforwardModel", "model.linear_small.FeedforwardModelSmall", "model.linear_attn_ck.FeedforwardModelAttnCK", "model.linear_mt.FeedforwardModelMT", "model.linear_mt_attn_ck.FeedforwardModelMTAttnCK"  (wrong-import-order)
trainer.py:20:0: C0411: third party import "yaml" should be placed before first party imports "model.linear.FeedforwardModel", "model.linear_small.FeedforwardModelSmall", "model.linear_attn_ck.FeedforwardModelAttnCK", "model.linear_mt.FeedforwardModelMT", "model.linear_mt_attn_ck.FeedforwardModelMTAttnCK"  (wrong-import-order)
trainer.py:21:0: C0411: third party import "omegaconf.DictConfig" should be placed before first party imports "model.linear.FeedforwardModel", "model.linear_small.FeedforwardModelSmall", "model.linear_attn_ck.FeedforwardModelAttnCK", "model.linear_mt.FeedforwardModelMT", "model.linear_mt_attn_ck.FeedforwardModelMTAttnCK"  (wrong-import-order)
trainer.py:23:0: C0411: third party import "torch" should be placed before first party imports "model.linear.FeedforwardModel", "model.linear_small.FeedforwardModelSmall", "model.linear_attn_ck.FeedforwardModelAttnCK", "model.linear_mt.FeedforwardModelMT", "model.linear_mt_attn_ck.FeedforwardModelMTAttnCK"  (wrong-import-order)
trainer.py:24:0: C0411: third party import "torch.distributed.all_gather" should be placed before first party imports "model.linear.FeedforwardModel", "model.linear_small.FeedforwardModelSmall", "model.linear_attn_ck.FeedforwardModelAttnCK", "model.linear_mt.FeedforwardModelMT", "model.linear_mt_attn_ck.FeedforwardModelMTAttnCK"  (wrong-import-order)
trainer.py:26:0: C0411: third party import "torch_optimizer.RAdam" should be placed before first party imports "model.linear.FeedforwardModel", "model.linear_small.FeedforwardModelSmall", "model.linear_attn_ck.FeedforwardModelAttnCK", "model.linear_mt.FeedforwardModelMT", "model.linear_mt_attn_ck.FeedforwardModelMTAttnCK"  (wrong-import-order)
trainer.py:23:0: C0412: Imports from package torch are not grouped (ungrouped-imports)
trainer.py:1:0: W0611: Unused import os (unused-import)
trainer.py:4:0: W0611: Unused torch.nn.functional imported as F (unused-import)
trainer.py:6:0: W0611: Unused metrics imported from sklearn (unused-import)
trainer.py:7:0: W0611: Unused AutoModelForAudioClassification imported from transformers (unused-import)
trainer.py:9:0: W0611: Unused OrderedDict imported from collections (unused-import)
trainer.py:10:0: W0611: Unused MaxMetric imported from torchmetrics (unused-import)
trainer.py:10:0: W0611: Unused Accuracy imported from torchmetrics (unused-import)
trainer.py:14:0: W0611: Unused FeedforwardModelSmall imported from model.linear_small (unused-import)
trainer.py:16:0: W0611: Unused FeedforwardModelMT imported from model.linear_mt (unused-import)
trainer.py:20:0: W0611: Unused import yaml (unused-import)
trainer.py:26:0: W0611: Unused RAdam imported from torch_optimizer (unused-import)
************* Module Music2Emotion.dataset_loaders.deam
dataset_loaders\deam.py:26:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\deam.py:59:0: C0301: Line too long (105/100) (line-too-long)
dataset_loaders\deam.py:64:52: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\deam.py:77:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\deam.py:80:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\deam.py:89:31: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\deam.py:93:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\deam.py:124:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\deam.py:126:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\deam.py:130:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\deam.py:140:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\deam.py:147:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\deam.py:150:0: C0301: Line too long (103/100) (line-too-long)
dataset_loaders\deam.py:151:0: C0301: Line too long (103/100) (line-too-long)
dataset_loaders\deam.py:153:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\deam.py:154:57: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\deam.py:168:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\deam.py:201:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\deam.py:204:0: C0301: Line too long (113/100) (line-too-long)
dataset_loaders\deam.py:216:0: C0301: Line too long (107/100) (line-too-long)
dataset_loaders\deam.py:220:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\deam.py:236:0: C0305: Trailing newlines (trailing-newlines)
dataset_loaders\deam.py:1:0: C0114: Missing module docstring (missing-module-docstring)
dataset_loaders\deam.py:10:0: E0401: Unable to import 'music2latent' (import-error)
dataset_loaders\deam.py:16:0: C0115: Missing class docstring (missing-class-docstring)
dataset_loaders\deam.py:62:12: C0103: Attribute name "chordRootDic" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\deam.py:64:12: C0103: Attribute name "chordAttrDic" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\deam.py:16:0: R0902: Too many instance attributes (21/7) (too-many-instance-attributes)
dataset_loaders\deam.py:28:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\deam.py:55:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\deam.py:57:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\deam.py:61:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\deam.py:63:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\deam.py:71:4: R0914: Too many local variables (49/15) (too-many-locals)
dataset_loaders\deam.py:97:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\deam.py:109:16: C0103: Variable name "chordRootID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\deam.py:111:20: C0103: Variable name "chordAttrID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\deam.py:113:20: C0103: Variable name "chordAttrID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\deam.py:115:16: C0103: Variable name "chordRootID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\deam.py:116:16: C0103: Variable name "chordAttrID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\deam.py:117:32: E0606: Possibly using variable 'chordRootID' before assignment (possibly-used-before-assignment)
dataset_loaders\deam.py:118:32: E0606: Possibly using variable 'chordAttrID' before assignment (possibly-used-before-assignment)
dataset_loaders\deam.py:161:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\deam.py:71:4: R0912: Too many branches (24/12) (too-many-branches)
dataset_loaders\deam.py:71:4: R0915: Too many statements (94/50) (too-many-statements)
dataset_loaders\deam.py:176:8: W0612: Unused variable 'embeddings' (unused-variable)
dataset_loaders\deam.py:3:0: C0411: standard import "pickle" should be placed before third party import "numpy" (wrong-import-order)
dataset_loaders\deam.py:8:0: C0411: standard import "csv" should be placed before third party imports "numpy", "torch.utils.data", "torchaudio.transforms", "torchaudio", "torch" (wrong-import-order)
dataset_loaders\deam.py:11:0: C0411: standard import "json" should be placed before third party imports "numpy", "torch.utils.data", "torchaudio.transforms" (...) "torch", "pytorch_lightning", "music2latent.EncoderDecoder" (wrong-import-order)
dataset_loaders\deam.py:12:0: C0411: standard import "math" should be placed before third party imports "numpy", "torch.utils.data", "torchaudio.transforms" (...) "torch", "pytorch_lightning", "music2latent.EncoderDecoder" (wrong-import-order)
dataset_loaders\deam.py:3:0: W0611: Unused import pickle (unused-import)
dataset_loaders\deam.py:5:0: W0611: Unused torchaudio.transforms imported as T (unused-import)
dataset_loaders\deam.py:6:0: W0611: Unused import torchaudio (unused-import)
dataset_loaders\deam.py:8:0: W0611: Unused import csv (unused-import)
dataset_loaders\deam.py:9:0: W0611: Unused pytorch_lightning imported as pl (unused-import)
dataset_loaders\deam.py:10:0: W0611: Unused EncoderDecoder imported from music2latent (unused-import)
dataset_loaders\deam.py:12:0: W0611: Unused import math (unused-import)
dataset_loaders\deam.py:13:0: W0611: Unused StandardScaler imported from sklearn.preprocessing (unused-import)
************* Module Music2Emotion.dataset_loaders.emomusic
dataset_loaders\emomusic.py:26:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\emomusic.py:46:0: C0301: Line too long (105/100) (line-too-long)
dataset_loaders\emomusic.py:51:52: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\emomusic.py:79:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\emomusic.py:88:31: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\emomusic.py:92:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\emomusic.py:123:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\emomusic.py:125:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\emomusic.py:129:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\emomusic.py:139:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\emomusic.py:146:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\emomusic.py:149:0: C0301: Line too long (103/100) (line-too-long)
dataset_loaders\emomusic.py:150:0: C0301: Line too long (103/100) (line-too-long)
dataset_loaders\emomusic.py:152:57: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\emomusic.py:166:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\emomusic.py:198:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\emomusic.py:201:0: C0301: Line too long (113/100) (line-too-long)
dataset_loaders\emomusic.py:213:0: C0301: Line too long (107/100) (line-too-long)
dataset_loaders\emomusic.py:217:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\emomusic.py:234:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\emomusic.py:235:0: C0305: Trailing newlines (trailing-newlines)
dataset_loaders\emomusic.py:1:0: C0114: Missing module docstring (missing-module-docstring)
dataset_loaders\emomusic.py:10:0: E0401: Unable to import 'music2latent' (import-error)
dataset_loaders\emomusic.py:16:0: C0115: Missing class docstring (missing-class-docstring)
dataset_loaders\emomusic.py:49:12: C0103: Attribute name "chordRootDic" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\emomusic.py:51:12: C0103: Attribute name "chordAttrDic" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\emomusic.py:16:0: R0902: Too many instance attributes (21/7) (too-many-instance-attributes)
dataset_loaders\emomusic.py:28:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\emomusic.py:42:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\emomusic.py:44:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\emomusic.py:48:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\emomusic.py:50:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\emomusic.py:70:4: R0914: Too many local variables (49/15) (too-many-locals)
dataset_loaders\emomusic.py:96:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\emomusic.py:108:16: C0103: Variable name "chordRootID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\emomusic.py:110:20: C0103: Variable name "chordAttrID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\emomusic.py:112:20: C0103: Variable name "chordAttrID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\emomusic.py:114:16: C0103: Variable name "chordRootID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\emomusic.py:115:16: C0103: Variable name "chordAttrID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\emomusic.py:116:32: E0606: Possibly using variable 'chordRootID' before assignment (possibly-used-before-assignment)
dataset_loaders\emomusic.py:117:32: E0606: Possibly using variable 'chordAttrID' before assignment (possibly-used-before-assignment)
dataset_loaders\emomusic.py:159:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\emomusic.py:70:4: R0912: Too many branches (24/12) (too-many-branches)
dataset_loaders\emomusic.py:70:4: R0915: Too many statements (94/50) (too-many-statements)
dataset_loaders\emomusic.py:173:8: W0612: Unused variable 'embeddings' (unused-variable)
dataset_loaders\emomusic.py:3:0: C0411: standard import "pickle" should be placed before third party import "numpy" (wrong-import-order)
dataset_loaders\emomusic.py:8:0: C0411: standard import "csv" should be placed before third party imports "numpy", "torch.utils.data", "torchaudio.transforms", "torchaudio", "torch" (wrong-import-order)
dataset_loaders\emomusic.py:11:0: C0411: standard import "json" should be placed before third party imports "numpy", "torch.utils.data", "torchaudio.transforms" (...) "torch", "pytorch_lightning", "music2latent.EncoderDecoder" (wrong-import-order)
dataset_loaders\emomusic.py:12:0: C0411: standard import "math" should be placed before third party imports "numpy", "torch.utils.data", "torchaudio.transforms" (...) "torch", "pytorch_lightning", "music2latent.EncoderDecoder" (wrong-import-order)
dataset_loaders\emomusic.py:3:0: W0611: Unused import pickle (unused-import)
dataset_loaders\emomusic.py:5:0: W0611: Unused torchaudio.transforms imported as T (unused-import)
dataset_loaders\emomusic.py:6:0: W0611: Unused import torchaudio (unused-import)
dataset_loaders\emomusic.py:8:0: W0611: Unused import csv (unused-import)
dataset_loaders\emomusic.py:9:0: W0611: Unused pytorch_lightning imported as pl (unused-import)
dataset_loaders\emomusic.py:10:0: W0611: Unused EncoderDecoder imported from music2latent (unused-import)
dataset_loaders\emomusic.py:12:0: W0611: Unused import math (unused-import)
dataset_loaders\emomusic.py:13:0: W0611: Unused StandardScaler imported from sklearn.preprocessing (unused-import)
************* Module Music2Emotion.dataset_loaders.jamendo
dataset_loaders\jamendo.py:44:0: C0301: Line too long (111/100) (line-too-long)
dataset_loaders\jamendo.py:55:0: C0301: Line too long (105/100) (line-too-long)
dataset_loaders\jamendo.py:56:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\jamendo.py:60:52: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\jamendo.py:62:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\jamendo.py:66:0: C0301: Line too long (125/100) (line-too-long)
dataset_loaders\jamendo.py:71:42: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\jamendo.py:89:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\jamendo.py:120:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\jamendo.py:122:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\jamendo.py:126:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\jamendo.py:136:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\jamendo.py:143:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\jamendo.py:146:0: C0301: Line too long (103/100) (line-too-long)
dataset_loaders\jamendo.py:147:0: C0301: Line too long (103/100) (line-too-long)
dataset_loaders\jamendo.py:149:57: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\jamendo.py:194:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\jamendo.py:197:0: C0301: Line too long (113/100) (line-too-long)
dataset_loaders\jamendo.py:209:0: C0301: Line too long (107/100) (line-too-long)
dataset_loaders\jamendo.py:228:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\jamendo.py:1:0: C0114: Missing module docstring (missing-module-docstring)
dataset_loaders\jamendo.py:10:0: E0401: Unable to import 'music2latent' (import-error)
dataset_loaders\jamendo.py:16:0: C0115: Missing class docstring (missing-class-docstring)
dataset_loaders\jamendo.py:58:12: C0103: Attribute name "chordRootDic" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\jamendo.py:60:12: C0103: Attribute name "chordAttrDic" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\jamendo.py:16:0: R0902: Too many instance attributes (22/7) (too-many-instance-attributes)
dataset_loaders\jamendo.py:51:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\jamendo.py:53:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\jamendo.py:57:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\jamendo.py:59:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\jamendo.py:68:4: R0914: Too many local variables (48/15) (too-many-locals)
dataset_loaders\jamendo.py:93:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\jamendo.py:105:16: C0103: Variable name "chordRootID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\jamendo.py:107:20: C0103: Variable name "chordAttrID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\jamendo.py:109:20: C0103: Variable name "chordAttrID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\jamendo.py:111:16: C0103: Variable name "chordRootID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\jamendo.py:112:16: C0103: Variable name "chordAttrID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\jamendo.py:113:32: E0606: Possibly using variable 'chordRootID' before assignment (possibly-used-before-assignment)
dataset_loaders\jamendo.py:114:32: E0606: Possibly using variable 'chordAttrID' before assignment (possibly-used-before-assignment)
dataset_loaders\jamendo.py:156:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\jamendo.py:68:4: R0912: Too many branches (22/12) (too-many-branches)
dataset_loaders\jamendo.py:68:4: R0915: Too many statements (91/50) (too-many-statements)
dataset_loaders\jamendo.py:169:8: W0612: Unused variable 'embeddings' (unused-variable)
dataset_loaders\jamendo.py:3:0: C0411: standard import "pickle" should be placed before third party import "numpy" (wrong-import-order)
dataset_loaders\jamendo.py:8:0: C0411: standard import "csv" should be placed before third party imports "numpy", "torch.utils.data", "torchaudio.transforms", "torchaudio", "torch" (wrong-import-order)
dataset_loaders\jamendo.py:11:0: C0411: standard import "json" should be placed before third party imports "numpy", "torch.utils.data", "torchaudio.transforms" (...) "torch", "pytorch_lightning", "music2latent.EncoderDecoder" (wrong-import-order)
dataset_loaders\jamendo.py:12:0: C0411: standard import "math" should be placed before third party imports "numpy", "torch.utils.data", "torchaudio.transforms" (...) "torch", "pytorch_lightning", "music2latent.EncoderDecoder" (wrong-import-order)
dataset_loaders\jamendo.py:5:0: W0611: Unused torchaudio.transforms imported as T (unused-import)
dataset_loaders\jamendo.py:6:0: W0611: Unused import torchaudio (unused-import)
dataset_loaders\jamendo.py:8:0: W0611: Unused import csv (unused-import)
dataset_loaders\jamendo.py:9:0: W0611: Unused pytorch_lightning imported as pl (unused-import)
dataset_loaders\jamendo.py:10:0: W0611: Unused EncoderDecoder imported from music2latent (unused-import)
dataset_loaders\jamendo.py:12:0: W0611: Unused import math (unused-import)
dataset_loaders\jamendo.py:13:0: W0611: Unused StandardScaler imported from sklearn.preprocessing (unused-import)
************* Module Music2Emotion.dataset_loaders.pmemo
dataset_loaders\pmemo.py:26:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\pmemo.py:45:0: C0301: Line too long (105/100) (line-too-long)
dataset_loaders\pmemo.py:49:52: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\pmemo.py:82:31: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\pmemo.py:86:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\pmemo.py:117:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\pmemo.py:119:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\pmemo.py:123:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\pmemo.py:133:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\pmemo.py:140:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\pmemo.py:143:0: C0301: Line too long (103/100) (line-too-long)
dataset_loaders\pmemo.py:144:0: C0301: Line too long (103/100) (line-too-long)
dataset_loaders\pmemo.py:160:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\pmemo.py:192:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\pmemo.py:195:0: C0301: Line too long (113/100) (line-too-long)
dataset_loaders\pmemo.py:207:0: C0301: Line too long (107/100) (line-too-long)
dataset_loaders\pmemo.py:211:0: C0303: Trailing whitespace (trailing-whitespace)
dataset_loaders\pmemo.py:226:0: C0304: Final newline missing (missing-final-newline)
dataset_loaders\pmemo.py:1:0: C0114: Missing module docstring (missing-module-docstring)
dataset_loaders\pmemo.py:10:0: E0401: Unable to import 'music2latent' (import-error)
dataset_loaders\pmemo.py:16:0: C0115: Missing class docstring (missing-class-docstring)
dataset_loaders\pmemo.py:47:12: C0103: Attribute name "chordRootDic" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\pmemo.py:49:12: C0103: Attribute name "chordAttrDic" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\pmemo.py:16:0: R0902: Too many instance attributes (21/7) (too-many-instance-attributes)
dataset_loaders\pmemo.py:28:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\pmemo.py:41:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\pmemo.py:43:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\pmemo.py:46:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\pmemo.py:48:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\pmemo.py:66:4: R0914: Too many local variables (49/15) (too-many-locals)
dataset_loaders\pmemo.py:90:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\pmemo.py:102:16: C0103: Variable name "chordRootID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\pmemo.py:104:20: C0103: Variable name "chordAttrID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\pmemo.py:106:20: C0103: Variable name "chordAttrID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\pmemo.py:108:16: C0103: Variable name "chordRootID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\pmemo.py:109:16: C0103: Variable name "chordAttrID" doesn't conform to snake_case naming style (invalid-name)
dataset_loaders\pmemo.py:110:32: E0606: Possibly using variable 'chordRootID' before assignment (possibly-used-before-assignment)
dataset_loaders\pmemo.py:111:32: E0606: Possibly using variable 'chordAttrID' before assignment (possibly-used-before-assignment)
dataset_loaders\pmemo.py:153:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
dataset_loaders\pmemo.py:66:4: R0912: Too many branches (24/12) (too-many-branches)
dataset_loaders\pmemo.py:66:4: R0915: Too many statements (94/50) (too-many-statements)
dataset_loaders\pmemo.py:167:8: W0612: Unused variable 'embeddings' (unused-variable)
dataset_loaders\pmemo.py:3:0: C0411: standard import "pickle" should be placed before third party import "numpy" (wrong-import-order)
dataset_loaders\pmemo.py:8:0: C0411: standard import "csv" should be placed before third party imports "numpy", "torch.utils.data", "torchaudio.transforms", "torchaudio", "torch" (wrong-import-order)
dataset_loaders\pmemo.py:11:0: C0411: standard import "json" should be placed before third party imports "numpy", "torch.utils.data", "torchaudio.transforms" (...) "torch", "pytorch_lightning", "music2latent.EncoderDecoder" (wrong-import-order)
dataset_loaders\pmemo.py:12:0: C0411: standard import "math" should be placed before third party imports "numpy", "torch.utils.data", "torchaudio.transforms" (...) "torch", "pytorch_lightning", "music2latent.EncoderDecoder" (wrong-import-order)
dataset_loaders\pmemo.py:3:0: W0611: Unused import pickle (unused-import)
dataset_loaders\pmemo.py:5:0: W0611: Unused torchaudio.transforms imported as T (unused-import)
dataset_loaders\pmemo.py:6:0: W0611: Unused import torchaudio (unused-import)
dataset_loaders\pmemo.py:8:0: W0611: Unused import csv (unused-import)
dataset_loaders\pmemo.py:9:0: W0611: Unused pytorch_lightning imported as pl (unused-import)
dataset_loaders\pmemo.py:10:0: W0611: Unused EncoderDecoder imported from music2latent (unused-import)
dataset_loaders\pmemo.py:12:0: W0611: Unused import math (unused-import)
dataset_loaders\pmemo.py:13:0: W0611: Unused StandardScaler imported from sklearn.preprocessing (unused-import)
************* Module Music2Emotion.dataset_loaders.__init__
dataset_loaders\__init__.py:3:19: C0303: Trailing whitespace (trailing-whitespace)
************* Module Music2Emotion.model.linear
model\linear.py:13:40: C0303: Trailing whitespace (trailing-whitespace)
model\linear.py:14:33: C0303: Trailing whitespace (trailing-whitespace)
model\linear.py:16:28: C0303: Trailing whitespace (trailing-whitespace)
model\linear.py:19:32: C0303: Trailing whitespace (trailing-whitespace)
model\linear.py:24:32: C0303: Trailing whitespace (trailing-whitespace)
model\linear.py:29:32: C0303: Trailing whitespace (trailing-whitespace)
model\linear.py:39:0: C0303: Trailing whitespace (trailing-whitespace)
model\linear.py:1:0: C0114: Missing module docstring (missing-module-docstring)
model\linear.py:3:0: R0402: Use 'from torch import nn' instead (consider-using-from-import)
model\linear.py:9:0: C0115: Missing class docstring (missing-class-docstring)
model\linear.py:11:8: R1725: Consider using Python 3 style super() without arguments (super-with-arguments)
model\linear.py:36:4: C0116: Missing function or method docstring (missing-function-docstring)
model\linear.py:1:0: W0611: Unused import os (unused-import)
model\linear.py:2:0: W0611: Unused import torch (unused-import)
model\linear.py:4:0: W0611: Unused pytorch_lightning imported as pl (unused-import)
model\linear.py:5:0: W0611: Unused metrics imported from sklearn (unused-import)
model\linear.py:6:0: W0611: Unused AutoModelForAudioClassification imported from transformers (unused-import)
model\linear.py:7:0: W0611: Unused numpy imported as np (unused-import)
************* Module Music2Emotion.model.linear_attn_ck
model\linear_attn_ck.py:24:88: C0303: Trailing whitespace (trailing-whitespace)
model\linear_attn_ck.py:25:85: C0303: Trailing whitespace (trailing-whitespace)
model\linear_attn_ck.py:38:0: C0303: Trailing whitespace (trailing-whitespace)
model\linear_attn_ck.py:44:0: C0301: Line too long (138/100) (line-too-long)
model\linear_attn_ck.py:61:42: C0303: Trailing whitespace (trailing-whitespace)
model\linear_attn_ck.py:68:0: C0301: Line too long (122/100) (line-too-long)
model\linear_attn_ck.py:69:0: C0301: Line too long (122/100) (line-too-long)
model\linear_attn_ck.py:70:0: C0303: Trailing whitespace (trailing-whitespace)
model\linear_attn_ck.py:81:114: C0303: Trailing whitespace (trailing-whitespace)
model\linear_attn_ck.py:81:0: C0301: Line too long (114/100) (line-too-long)
model\linear_attn_ck.py:82:0: C0301: Line too long (135/100) (line-too-long)
model\linear_attn_ck.py:84:0: C0301: Line too long (102/100) (line-too-long)
model\linear_attn_ck.py:85:0: C0303: Trailing whitespace (trailing-whitespace)
model\linear_attn_ck.py:90:0: C0303: Trailing whitespace (trailing-whitespace)
model\linear_attn_ck.py:92:0: C0304: Final newline missing (missing-final-newline)
model\linear_attn_ck.py:1:0: C0114: Missing module docstring (missing-module-docstring)
model\linear_attn_ck.py:3:0: R0402: Use 'from torch import nn' instead (consider-using-from-import)
model\linear_attn_ck.py:9:0: C0115: Missing class docstring (missing-class-docstring)
model\linear_attn_ck.py:19:4: C0116: Missing function or method docstring (missing-function-docstring)
model\linear_attn_ck.py:23:0: C0115: Missing class docstring (missing-class-docstring)
model\linear_attn_ck.py:23:0: R0902: Too many instance attributes (8/7) (too-many-instance-attributes)
model\linear_attn_ck.py:24:4: R0913: Too many arguments (14/5) (too-many-arguments)
model\linear_attn_ck.py:24:4: R0917: Too many positional arguments (14/5) (too-many-positional-arguments)
model\linear_attn_ck.py:24:57: W0613: Unused argument 'num_layers' (unused-argument)
model\linear_attn_ck.py:24:71: W0613: Unused argument 'dropout_rate' (unused-argument)
model\linear_attn_ck.py:25:17: W0613: Unused argument 'num_key' (unused-argument)
model\linear_attn_ck.py:25:30: W0613: Unused argument 'num_chords' (unused-argument)
model\linear_attn_ck.py:26:17: W0613: Unused argument 'key_emb_dim' (unused-argument)
model\linear_attn_ck.py:26:32: W0613: Unused argument 'chord_emb_dim' (unused-argument)
model\linear_attn_ck.py:60:4: C0116: Missing function or method docstring (missing-function-docstring)
model\linear_attn_ck.py:60:4: R0914: Too many local variables (16/15) (too-many-locals)
model\linear_attn_ck.py:1:0: W0611: Unused import os (unused-import)
model\linear_attn_ck.py:4:0: W0611: Unused pytorch_lightning imported as pl (unused-import)
model\linear_attn_ck.py:5:0: W0611: Unused metrics imported from sklearn (unused-import)
model\linear_attn_ck.py:6:0: W0611: Unused AutoModelForAudioClassification imported from transformers (unused-import)
************* Module Music2Emotion.model.linear_mt_attn_ck
model\linear_mt_attn_ck.py:24:127: C0303: Trailing whitespace (trailing-whitespace)
model\linear_mt_attn_ck.py:24:0: C0301: Line too long (127/100) (line-too-long)
model\linear_mt_attn_ck.py:25:85: C0303: Trailing whitespace (trailing-whitespace)
model\linear_mt_attn_ck.py:38:0: C0303: Trailing whitespace (trailing-whitespace)
model\linear_mt_attn_ck.py:44:0: C0301: Line too long (138/100) (line-too-long)
model\linear_mt_attn_ck.py:47:0: C0303: Trailing whitespace (trailing-whitespace)
model\linear_mt_attn_ck.py:59:0: C0303: Trailing whitespace (trailing-whitespace)
model\linear_mt_attn_ck.py:69:42: C0303: Trailing whitespace (trailing-whitespace)
model\linear_mt_attn_ck.py:76:0: C0301: Line too long (122/100) (line-too-long)
model\linear_mt_attn_ck.py:77:0: C0301: Line too long (122/100) (line-too-long)
model\linear_mt_attn_ck.py:78:0: C0303: Trailing whitespace (trailing-whitespace)
model\linear_mt_attn_ck.py:87:114: C0303: Trailing whitespace (trailing-whitespace)
model\linear_mt_attn_ck.py:87:0: C0301: Line too long (114/100) (line-too-long)
model\linear_mt_attn_ck.py:88:0: C0301: Line too long (135/100) (line-too-long)
model\linear_mt_attn_ck.py:90:0: C0301: Line too long (102/100) (line-too-long)
model\linear_mt_attn_ck.py:91:0: C0303: Trailing whitespace (trailing-whitespace)
model\linear_mt_attn_ck.py:99:0: C0303: Trailing whitespace (trailing-whitespace)
model\linear_mt_attn_ck.py:100:0: C0304: Final newline missing (missing-final-newline)
model\linear_mt_attn_ck.py:1:0: C0114: Missing module docstring (missing-module-docstring)
model\linear_mt_attn_ck.py:3:0: R0402: Use 'from torch import nn' instead (consider-using-from-import)
model\linear_mt_attn_ck.py:9:0: C0115: Missing class docstring (missing-class-docstring)
model\linear_mt_attn_ck.py:19:4: C0116: Missing function or method docstring (missing-function-docstring)
model\linear_mt_attn_ck.py:23:0: C0115: Missing class docstring (missing-class-docstring)
model\linear_mt_attn_ck.py:23:0: R0902: Too many instance attributes (9/7) (too-many-instance-attributes)
model\linear_mt_attn_ck.py:24:4: R0913: Too many arguments (15/5) (too-many-arguments)
model\linear_mt_attn_ck.py:24:4: R0917: Too many positional arguments (15/5) (too-many-positional-arguments)
model\linear_mt_attn_ck.py:24:96: W0613: Unused argument 'num_layers' (unused-argument)
model\linear_mt_attn_ck.py:24:110: W0613: Unused argument 'dropout_rate' (unused-argument)
model\linear_mt_attn_ck.py:25:17: W0613: Unused argument 'num_key' (unused-argument)
model\linear_mt_attn_ck.py:25:30: W0613: Unused argument 'num_chords' (unused-argument)
model\linear_mt_attn_ck.py:26:17: W0613: Unused argument 'key_emb_dim' (unused-argument)
model\linear_mt_attn_ck.py:26:32: W0613: Unused argument 'chord_emb_dim' (unused-argument)
model\linear_mt_attn_ck.py:68:4: C0116: Missing function or method docstring (missing-function-docstring)
model\linear_mt_attn_ck.py:68:4: R0914: Too many local variables (17/15) (too-many-locals)
model\linear_mt_attn_ck.py:1:0: W0611: Unused import os (unused-import)
model\linear_mt_attn_ck.py:4:0: W0611: Unused pytorch_lightning imported as pl (unused-import)
model\linear_mt_attn_ck.py:5:0: W0611: Unused metrics imported from sklearn (unused-import)
model\linear_mt_attn_ck.py:6:0: W0611: Unused AutoModelForAudioClassification imported from transformers (unused-import)
************* Module Music2Emotion.model.__init__
model\__init__.py:3:19: C0303: Trailing whitespace (trailing-whitespace)
************* Module Music2Emotion.utils.btc_model
utils\btc_model.py:9:0: C0301: Line too long (114/100) (line-too-long)
utils\btc_model.py:13:0: C0301: Line too long (171/100) (line-too-long)
utils\btc_model.py:14:0: C0301: Line too long (159/100) (line-too-long)
utils\btc_model.py:48:0: C0301: Line too long (107/100) (line-too-long)
utils\btc_model.py:101:0: C0301: Line too long (110/100) (line-too-long)
utils\btc_model.py:117:0: C0301: Line too long (115/100) (line-too-long)
utils\btc_model.py:159:0: C0301: Line too long (146/100) (line-too-long)
utils\btc_model.py:198:0: C0305: Trailing newlines (trailing-newlines)
utils\btc_model.py:1:0: C0114: Missing module docstring (missing-module-docstring)
utils\btc_model.py:1:0: W0401: Wildcard import Music2Emotion.utils.transformer_modules (wildcard-import)
utils\btc_model.py:7:0: C0115: Missing class docstring (missing-class-docstring)
utils\btc_model.py:7:0: C0103: Class name "self_attention_block" doesn't conform to PascalCase naming style (invalid-name)
utils\btc_model.py:8:4: R0913: Too many arguments (11/5) (too-many-arguments)
utils\btc_model.py:8:4: R0917: Too many positional arguments (11/5) (too-many-positional-arguments)
utils\btc_model.py:10:8: R1725: Consider using Python 3 style super() without arguments (super-with-arguments)
utils\btc_model.py:19:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\btc_model.py:47:0: C0115: Missing class docstring (missing-class-docstring)
utils\btc_model.py:47:0: C0103: Class name "bi_directional_self_attention" doesn't conform to PascalCase naming style (invalid-name)
utils\btc_model.py:48:4: R0913: Too many arguments (10/5) (too-many-arguments)
utils\btc_model.py:48:4: R0917: Too many positional arguments (10/5) (too-many-positional-arguments)
utils\btc_model.py:51:8: R1725: Consider using Python 3 style super() without arguments (super-with-arguments)
utils\btc_model.py:53:28: R1734: Consider using [] instead of list() (use-list-literal)
utils\btc_model.py:83:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\btc_model.py:84:11: W0622: Redefining built-in 'list' (redefined-builtin)
utils\btc_model.py:100:0: C0115: Missing class docstring (missing-class-docstring)
utils\btc_model.py:100:0: C0103: Class name "bi_directional_self_attention_layers" doesn't conform to PascalCase naming style (invalid-name)
utils\btc_model.py:101:4: R0913: Too many arguments (13/5) (too-many-arguments)
utils\btc_model.py:101:4: R0917: Too many positional arguments (13/5) (too-many-positional-arguments)
utils\btc_model.py:104:8: R1725: Consider using Python 3 style super() without arguments (super-with-arguments)
utils\btc_model.py:121:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\btc_model.py:132:11: W0621: Redefining name 'weights_list' from outer scope (line 194) (redefined-outer-name)
utils\btc_model.py:138:0: C0115: Missing class docstring (missing-class-docstring)
utils\btc_model.py:138:0: C0103: Class name "BTC_model" doesn't conform to PascalCase naming style (invalid-name)
utils\btc_model.py:139:23: W0621: Redefining name 'config' from outer scope (line 181) (redefined-outer-name)
utils\btc_model.py:140:8: R1725: Consider using Python 3 style super() without arguments (super-with-arguments)
utils\btc_model.py:161:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\btc_model.py:164:26: W0621: Redefining name 'weights_list' from outer scope (line 194) (redefined-outer-name)
utils\btc_model.py:172:8: W0621: Redefining name 'prediction' from outer scope (line 194) (redefined-outer-name)
utils\btc_model.py:172:19: W0621: Redefining name 'second' from outer scope (line 194) (redefined-outer-name)
utils\btc_model.py:177:8: W0621: Redefining name 'loss' from outer scope (line 194) (redefined-outer-name)
utils\btc_model.py:184:4: C0103: Constant name "batch_size" doesn't conform to UPPER_CASE naming style (invalid-name)
utils\btc_model.py:185:4: C0103: Constant name "timestep" doesn't conform to UPPER_CASE naming style (invalid-name)
utils\btc_model.py:186:4: C0103: Constant name "feature_size" doesn't conform to UPPER_CASE naming style (invalid-name)
utils\btc_model.py:187:4: C0103: Constant name "num_chords" doesn't conform to UPPER_CASE naming style (invalid-name)
utils\btc_model.py:192:29: E1101: Instance of 'HParams' has no 'model' member (no-member)
utils\btc_model.py:1:0: W0614: Unused import(s) F, np, math, OutputLayer and Conv from wildcard import of Music2Emotion.utils.transformer_modules (unused-wildcard-import)
************* Module Music2Emotion.utils.chords
utils\chords.py:465:0: C0301: Line too long (103/100) (line-too-long)
utils\chords.py:512:0: C0301: Line too long (107/100) (line-too-long)
utils\chords.py:540:0: C0301: Line too long (127/100) (line-too-long)
utils\chords.py:542:0: C0305: Trailing newlines (trailing-newlines)
utils\chords.py:52:0: C0116: Missing function or method docstring (missing-function-docstring)
utils\chords.py:53:4: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
utils\chords.py:56:15: W1406: The u prefix for strings is no longer necessary in Python >=3.0 (redundant-u-string-prefix)
utils\chords.py:63:0: C0115: Missing class docstring (missing-class-docstring)
utils\chords.py:124:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\chords.py:125:30: C0321: More than one statement on a single line (multiple-statements)
utils\chords.py:126:30: C0321: More than one statement on a single line (multiple-statements)
utils\chords.py:127:31: C0321: More than one statement on a single line (multiple-statements)
utils\chords.py:128:31: C0321: More than one statement on a single line (multiple-statements)
utils\chords.py:191:15: W0718: Catching too general exception Exception (broad-exception-caught)
utils\chords.py:225:33: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\chords.py:247:4: R1710: Either all return statements in a function should return an expression, or none of them should. (inconsistent-return-statements)
utils\chords.py:355:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
utils\chords.py:442:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\chords.py:443:8: R1705: Unnecessary "else" after "return", remove the "else" and de-indent the code inside it (no-else-return)
utils\chords.py:446:12: R1705: Unnecessary "else" after "return", remove the "else" and de-indent the code inside it (no-else-return)
utils\chords.py:451:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\chords.py:461:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\chords.py:468:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\chords.py:469:8: R1705: Unnecessary "else" after "return", remove the "else" and de-indent the code inside it (no-else-return)
utils\chords.py:472:12: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
utils\chords.py:468:4: R0911: Too many return statements (16/6) (too-many-return-statements)
utils\chords.py:468:4: R0912: Too many branches (16/12) (too-many-branches)
utils\chords.py:503:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\chords.py:503:4: R0914: Too many local variables (16/15) (too-many-locals)
utils\chords.py:510:15: R1734: Consider using [] instead of list() (use-list-literal)
utils\chords.py:508:9: W0612: Unused variable 'ref_intervals' (unused-variable)
utils\chords.py:512:12: W0612: Unused variable 'chord_root' (unused-variable)
utils\chords.py:512:33: W0612: Unused variable 'scale_degrees' (unused-variable)
utils\chords.py:512:48: W0612: Unused variable 'bass' (unused-variable)
utils\chords.py:513:24: W0612: Unused variable 'ivs' (unused-variable)
utils\chords.py:513:29: W0612: Unused variable 'is_major' (unused-variable)
utils\chords.py:522:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\chords.py:523:8: C0200: Consider using enumerate instead of iterating with range and len (consider-using-enumerate)
************* Module Music2Emotion.utils.constants
utils\constants.py:16:7: C0303: Trailing whitespace (trailing-whitespace)
utils\constants.py:20:7: C0303: Trailing whitespace (trailing-whitespace)
utils\constants.py:28:18: C0303: Trailing whitespace (trailing-whitespace)
utils\constants.py:29:18: C0303: Trailing whitespace (trailing-whitespace)
utils\constants.py:1:0: C0114: Missing module docstring (missing-module-docstring)
utils\constants.py:8:0: C0411: standard import "os" should be placed before third party import "torch" (wrong-import-order)
utils\constants.py:7:0: W0611: Unused import torch (unused-import)
utils\constants.py:8:0: W0611: Unused import os (unused-import)
************* Module Music2Emotion.utils.custom_early_stopping
utils\custom_early_stopping.py:41:0: C0301: Line too long (103/100) (line-too-long)
utils\custom_early_stopping.py:55:0: C0301: Line too long (124/100) (line-too-long)
utils\custom_early_stopping.py:93:0: C0304: Final newline missing (missing-final-newline)
utils\custom_early_stopping.py:1:0: C0114: Missing module docstring (missing-module-docstring)
utils\custom_early_stopping.py:7:0: C0115: Missing class docstring (missing-class-docstring)
utils\custom_early_stopping.py:7:0: R0902: Too many instance attributes (10/7) (too-many-instance-attributes)
utils\custom_early_stopping.py:8:4: R0913: Too many arguments (6/5) (too-many-arguments)
utils\custom_early_stopping.py:8:4: R0917: Too many positional arguments (6/5) (too-many-positional-arguments)
utils\custom_early_stopping.py:23:8: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
utils\custom_early_stopping.py:3:0: W0611: Unused pytorch_lightning imported as pl (unused-import)
************* Module Music2Emotion.utils.hparams
utils\hparams.py:24:0: C0301: Line too long (108/100) (line-too-long)
utils\hparams.py:4:1: W0511: TODO: add function should be changed (fixme)
utils\hparams.py:1:0: C0114: Missing module docstring (missing-module-docstring)
utils\hparams.py:5:0: C0115: Missing class docstring (missing-class-docstring)
utils\hparams.py:5:0: R0205: Class 'HParams' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
utils\hparams.py:10:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\hparams.py:14:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\hparams.py:18:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\hparams.py:19:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
utils\hparams.py:24:51: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\hparams.py:27:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\hparams.py:28:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
************* Module Music2Emotion.utils.logger
utils\logger.py:1:0: C0114: Missing module docstring (missing-module-docstring)
utils\logger.py:38:8: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\logger.py:51:0: C0116: Missing function or method docstring (missing-function-docstring)
utils\logger.py:55:0: C0116: Missing function or method docstring (missing-function-docstring)
utils\logger.py:59:0: C0116: Missing function or method docstring (missing-function-docstring)
utils\logger.py:63:0: C0116: Missing function or method docstring (missing-function-docstring)
utils\logger.py:67:0: C0116: Missing function or method docstring (missing-function-docstring)
utils\logger.py:71:0: C0116: Missing function or method docstring (missing-function-docstring)
************* Module Music2Emotion.utils.mert
utils\mert.py:14:0: C0303: Trailing whitespace (trailing-whitespace)
utils\mert.py:15:0: C0301: Line too long (103/100) (line-too-long)
utils\mert.py:16:0: C0301: Line too long (106/100) (line-too-long)
utils\mert.py:27:0: C0301: Line too long (107/100) (line-too-long)
utils\mert.py:32:0: C0304: Final newline missing (missing-final-newline)
utils\mert.py:1:0: C0114: Missing module docstring (missing-module-docstring)
utils\mert.py:5:0: C0115: Missing class docstring (missing-class-docstring)
utils\mert.py:18:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\mert.py:5:0: R0903: Too few public methods (1/2) (too-few-public-methods)
************* Module Music2Emotion.utils.mir_eval_modules
utils\mir_eval_modules.py:7:0: C0301: Line too long (106/100) (line-too-long)
utils\mir_eval_modules.py:8:0: C0301: Line too long (101/100) (line-too-long)
utils\mir_eval_modules.py:11:0: C0301: Line too long (124/100) (line-too-long)
utils\mir_eval_modules.py:35:0: C0301: Line too long (190/100) (line-too-long)
utils\mir_eval_modules.py:41:0: C0301: Line too long (184/100) (line-too-long)
utils\mir_eval_modules.py:50:0: C0301: Line too long (111/100) (line-too-long)
utils\mir_eval_modules.py:51:0: C0301: Line too long (106/100) (line-too-long)
utils\mir_eval_modules.py:54:0: C0301: Line too long (109/100) (line-too-long)
utils\mir_eval_modules.py:55:0: C0325: Unnecessary parens after 'if' keyword (superfluous-parens)
utils\mir_eval_modules.py:90:0: C0301: Line too long (114/100) (line-too-long)
utils\mir_eval_modules.py:91:0: C0301: Line too long (112/100) (line-too-long)
utils\mir_eval_modules.py:93:0: C0301: Line too long (110/100) (line-too-long)
utils\mir_eval_modules.py:94:0: C0301: Line too long (110/100) (line-too-long)
utils\mir_eval_modules.py:104:0: C0301: Line too long (114/100) (line-too-long)
utils\mir_eval_modules.py:105:0: C0301: Line too long (112/100) (line-too-long)
utils\mir_eval_modules.py:107:0: C0301: Line too long (110/100) (line-too-long)
utils\mir_eval_modules.py:108:0: C0301: Line too long (110/100) (line-too-long)
utils\mir_eval_modules.py:118:0: C0301: Line too long (114/100) (line-too-long)
utils\mir_eval_modules.py:119:0: C0301: Line too long (112/100) (line-too-long)
utils\mir_eval_modules.py:121:0: C0301: Line too long (110/100) (line-too-long)
utils\mir_eval_modules.py:122:0: C0301: Line too long (110/100) (line-too-long)
utils\mir_eval_modules.py:132:0: C0301: Line too long (114/100) (line-too-long)
utils\mir_eval_modules.py:133:0: C0301: Line too long (112/100) (line-too-long)
utils\mir_eval_modules.py:135:0: C0301: Line too long (110/100) (line-too-long)
utils\mir_eval_modules.py:136:0: C0301: Line too long (110/100) (line-too-long)
utils\mir_eval_modules.py:146:0: C0301: Line too long (114/100) (line-too-long)
utils\mir_eval_modules.py:147:0: C0301: Line too long (112/100) (line-too-long)
utils\mir_eval_modules.py:149:0: C0301: Line too long (110/100) (line-too-long)
utils\mir_eval_modules.py:150:0: C0301: Line too long (110/100) (line-too-long)
utils\mir_eval_modules.py:160:0: C0301: Line too long (114/100) (line-too-long)
utils\mir_eval_modules.py:161:0: C0301: Line too long (112/100) (line-too-long)
utils\mir_eval_modules.py:163:0: C0301: Line too long (110/100) (line-too-long)
utils\mir_eval_modules.py:164:0: C0301: Line too long (110/100) (line-too-long)
utils\mir_eval_modules.py:174:0: C0301: Line too long (114/100) (line-too-long)
utils\mir_eval_modules.py:175:0: C0301: Line too long (112/100) (line-too-long)
utils\mir_eval_modules.py:177:0: C0301: Line too long (110/100) (line-too-long)
utils\mir_eval_modules.py:178:0: C0301: Line too long (110/100) (line-too-long)
utils\mir_eval_modules.py:202:0: C0301: Line too long (123/100) (line-too-long)
utils\mir_eval_modules.py:205:0: C0301: Line too long (110/100) (line-too-long)
utils\mir_eval_modules.py:217:0: C0301: Line too long (107/100) (line-too-long)
utils\mir_eval_modules.py:233:0: C0301: Line too long (118/100) (line-too-long)
utils\mir_eval_modules.py:237:0: C0301: Line too long (172/100) (line-too-long)
utils\mir_eval_modules.py:245:0: C0301: Line too long (105/100) (line-too-long)
utils\mir_eval_modules.py:252:0: C0301: Line too long (109/100) (line-too-long)
utils\mir_eval_modules.py:262:0: C0301: Line too long (118/100) (line-too-long)
utils\mir_eval_modules.py:266:0: C0301: Line too long (108/100) (line-too-long)
utils\mir_eval_modules.py:276:0: C0301: Line too long (125/100) (line-too-long)
utils\mir_eval_modules.py:288:0: C0301: Line too long (107/100) (line-too-long)
utils\mir_eval_modules.py:304:0: C0301: Line too long (163/100) (line-too-long)
utils\mir_eval_modules.py:305:0: C0301: Line too long (122/100) (line-too-long)
utils\mir_eval_modules.py:315:0: C0301: Line too long (105/100) (line-too-long)
utils\mir_eval_modules.py:322:0: C0301: Line too long (109/100) (line-too-long)
utils\mir_eval_modules.py:332:0: C0301: Line too long (118/100) (line-too-long)
utils\mir_eval_modules.py:336:0: C0301: Line too long (108/100) (line-too-long)
utils\mir_eval_modules.py:347:0: C0301: Line too long (109/100) (line-too-long)
utils\mir_eval_modules.py:360:0: C0301: Line too long (107/100) (line-too-long)
utils\mir_eval_modules.py:376:0: C0301: Line too long (118/100) (line-too-long)
utils\mir_eval_modules.py:380:0: C0301: Line too long (172/100) (line-too-long)
utils\mir_eval_modules.py:388:0: C0301: Line too long (104/100) (line-too-long)
utils\mir_eval_modules.py:395:0: C0301: Line too long (108/100) (line-too-long)
utils\mir_eval_modules.py:404:0: C0301: Line too long (118/100) (line-too-long)
utils\mir_eval_modules.py:408:0: C0301: Line too long (108/100) (line-too-long)
utils\mir_eval_modules.py:418:0: C0301: Line too long (124/100) (line-too-long)
utils\mir_eval_modules.py:431:0: C0301: Line too long (107/100) (line-too-long)
utils\mir_eval_modules.py:447:0: C0301: Line too long (163/100) (line-too-long)
utils\mir_eval_modules.py:448:0: C0301: Line too long (122/100) (line-too-long)
utils\mir_eval_modules.py:458:0: C0301: Line too long (104/100) (line-too-long)
utils\mir_eval_modules.py:465:0: C0301: Line too long (108/100) (line-too-long)
utils\mir_eval_modules.py:474:0: C0301: Line too long (118/100) (line-too-long)
utils\mir_eval_modules.py:478:0: C0301: Line too long (108/100) (line-too-long)
utils\mir_eval_modules.py:1:0: C0114: Missing module docstring (missing-module-docstring)
utils\mir_eval_modules.py:13:0: C0116: Missing function or method docstring (missing-function-docstring)
utils\mir_eval_modules.py:14:4: W0621: Redefining name 'idx2voca_chord' from outer scope (line 13) (redefined-outer-name)
utils\mir_eval_modules.py:29:0: C0116: Missing function or method docstring (missing-function-docstring)
utils\mir_eval_modules.py:49:0: C0116: Missing function or method docstring (missing-function-docstring)
utils\mir_eval_modules.py:53:0: C0116: Missing function or method docstring (missing-function-docstring)
utils\mir_eval_modules.py:58:0: C0115: Missing class docstring (missing-class-docstring)
utils\mir_eval_modules.py:58:0: C0103: Class name "metrics" doesn't conform to PascalCase naming style (invalid-name)
utils\mir_eval_modules.py:60:8: R1725: Consider using Python 3 style super() without arguments (super-with-arguments)
utils\mir_eval_modules.py:62:31: R1735: Consider using '{}' instead of a call to 'dict'. (use-dict-literal)
utils\mir_eval_modules.py:64:38: R1734: Consider using [] instead of list() (use-list-literal)
utils\mir_eval_modules.py:65:29: R1735: Consider using '{}' instead of a call to 'dict'. (use-dict-literal)
utils\mir_eval_modules.py:67:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\mir_eval_modules.py:86:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\mir_eval_modules.py:100:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\mir_eval_modules.py:114:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\mir_eval_modules.py:128:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\mir_eval_modules.py:142:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\mir_eval_modules.py:156:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\mir_eval_modules.py:170:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\mir_eval_modules.py:184:0: C0116: Missing function or method docstring (missing-function-docstring)
utils\mir_eval_modules.py:185:4: C0200: Consider using enumerate instead of iterating with range and len (consider-using-enumerate)
utils\mir_eval_modules.py:205:0: C0116: Missing function or method docstring (missing-function-docstring)
utils\mir_eval_modules.py:205:0: R0913: Too many arguments (8/5) (too-many-arguments)
utils\mir_eval_modules.py:205:0: R0917: Too many positional arguments (8/5) (too-many-positional-arguments)
utils\mir_eval_modules.py:205:0: R0914: Too many local variables (37/15) (too-many-locals)
utils\mir_eval_modules.py:210:23: R1734: Consider using [] instead of list() (use-list-literal)
utils\mir_eval_modules.py:267:8: W0702: No exception type(s) specified (bare-except)
utils\mir_eval_modules.py:236:25: R1714: Consider merging these comparisons with 'in' by using 'model_type in ('cnn', 'crnn')'. Use a set instead if elements are hashable. (consider-using-in)
utils\mir_eval_modules.py:244:32: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\mir_eval_modules.py:251:36: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\mir_eval_modules.py:256:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
utils\mir_eval_modules.py:266:26: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\mir_eval_modules.py:268:18: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\mir_eval_modules.py:205:0: R0912: Too many branches (16/12) (too-many-branches)
utils\mir_eval_modules.py:205:0: R0915: Too many statements (52/50) (too-many-statements)
utils\mir_eval_modules.py:276:0: C0116: Missing function or method docstring (missing-function-docstring)
utils\mir_eval_modules.py:276:0: R0913: Too many arguments (9/5) (too-many-arguments)
utils\mir_eval_modules.py:276:0: R0917: Too many positional arguments (9/5) (too-many-positional-arguments)
utils\mir_eval_modules.py:276:0: R0914: Too many local variables (38/15) (too-many-locals)
utils\mir_eval_modules.py:281:23: R1734: Consider using [] instead of list() (use-list-literal)
utils\mir_eval_modules.py:337:8: W0702: No exception type(s) specified (bare-except)
utils\mir_eval_modules.py:303:23: R1714: Consider merging these comparisons with 'in' by using 'model_type in ('cnn', 'crnn', 'btc')'. Use a set instead if elements are hashable. (consider-using-in)
utils\mir_eval_modules.py:314:32: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\mir_eval_modules.py:321:36: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\mir_eval_modules.py:326:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
utils\mir_eval_modules.py:336:26: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\mir_eval_modules.py:338:18: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\mir_eval_modules.py:276:0: R0912: Too many branches (16/12) (too-many-branches)
utils\mir_eval_modules.py:276:0: R0915: Too many statements (51/50) (too-many-statements)
utils\mir_eval_modules.py:347:0: C0116: Missing function or method docstring (missing-function-docstring)
utils\mir_eval_modules.py:347:0: R0913: Too many arguments (8/5) (too-many-arguments)
utils\mir_eval_modules.py:347:0: R0917: Too many positional arguments (8/5) (too-many-positional-arguments)
utils\mir_eval_modules.py:347:0: R0914: Too many local variables (37/15) (too-many-locals)
utils\mir_eval_modules.py:353:23: R1734: Consider using [] instead of list() (use-list-literal)
utils\mir_eval_modules.py:409:8: W0702: No exception type(s) specified (bare-except)
utils\mir_eval_modules.py:379:25: R1714: Consider merging these comparisons with 'in' by using 'model_type in ('cnn', 'crnn')'. Use a set instead if elements are hashable. (consider-using-in)
utils\mir_eval_modules.py:387:32: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\mir_eval_modules.py:394:36: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\mir_eval_modules.py:399:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
utils\mir_eval_modules.py:408:26: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\mir_eval_modules.py:410:18: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\mir_eval_modules.py:347:0: R0912: Too many branches (16/12) (too-many-branches)
utils\mir_eval_modules.py:347:0: R0915: Too many statements (52/50) (too-many-statements)
utils\mir_eval_modules.py:418:0: C0116: Missing function or method docstring (missing-function-docstring)
utils\mir_eval_modules.py:418:0: R0913: Too many arguments (9/5) (too-many-arguments)
utils\mir_eval_modules.py:418:0: R0917: Too many positional arguments (9/5) (too-many-positional-arguments)
utils\mir_eval_modules.py:418:0: R0914: Too many local variables (38/15) (too-many-locals)
utils\mir_eval_modules.py:424:23: R1734: Consider using [] instead of list() (use-list-literal)
utils\mir_eval_modules.py:479:8: W0702: No exception type(s) specified (bare-except)
utils\mir_eval_modules.py:446:23: R1714: Consider merging these comparisons with 'in' by using 'model_type in ('cnn', 'crnn', 'btc')'. Use a set instead if elements are hashable. (consider-using-in)
utils\mir_eval_modules.py:457:32: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\mir_eval_modules.py:464:36: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\mir_eval_modules.py:469:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
utils\mir_eval_modules.py:478:26: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\mir_eval_modules.py:480:18: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\mir_eval_modules.py:418:0: R0912: Too many branches (16/12) (too-many-branches)
utils\mir_eval_modules.py:418:0: R0915: Too many statements (51/50) (too-many-statements)
utils\mir_eval_modules.py:5:0: C0411: standard import "os" should be placed before third party imports "numpy", "librosa", "mir_eval", "torch" (wrong-import-order)
************* Module Music2Emotion.utils.preprocess
utils\preprocess.py:20:0: C0301: Line too long (105/100) (line-too-long)
utils\preprocess.py:74:0: C0301: Line too long (121/100) (line-too-long)
utils\preprocess.py:90:0: C0301: Line too long (102/100) (line-too-long)
utils\preprocess.py:98:0: C0301: Line too long (113/100) (line-too-long)
utils\preprocess.py:109:0: C0301: Line too long (102/100) (line-too-long)
utils\preprocess.py:112:0: C0301: Line too long (121/100) (line-too-long)
utils\preprocess.py:113:0: C0301: Line too long (103/100) (line-too-long)
utils\preprocess.py:138:0: C0301: Line too long (141/100) (line-too-long)
utils\preprocess.py:219:0: C0301: Line too long (101/100) (line-too-long)
utils\preprocess.py:222:0: C0301: Line too long (106/100) (line-too-long)
utils\preprocess.py:224:0: C0301: Line too long (111/100) (line-too-long)
utils\preprocess.py:225:0: C0301: Line too long (136/100) (line-too-long)
utils\preprocess.py:229:0: C0301: Line too long (110/100) (line-too-long)
utils\preprocess.py:233:0: C0301: Line too long (102/100) (line-too-long)
utils\preprocess.py:235:0: C0301: Line too long (112/100) (line-too-long)
utils\preprocess.py:237:0: C0301: Line too long (118/100) (line-too-long)
utils\preprocess.py:265:0: C0301: Line too long (104/100) (line-too-long)
utils\preprocess.py:270:0: C0301: Line too long (107/100) (line-too-long)
utils\preprocess.py:271:0: C0301: Line too long (108/100) (line-too-long)
utils\preprocess.py:301:0: C0301: Line too long (102/100) (line-too-long)
utils\preprocess.py:347:0: C0301: Line too long (102/100) (line-too-long)
utils\preprocess.py:388:0: C0301: Line too long (157/100) (line-too-long)
utils\preprocess.py:390:0: C0301: Line too long (253/100) (line-too-long)
utils\preprocess.py:395:0: C0301: Line too long (117/100) (line-too-long)
utils\preprocess.py:397:0: C0301: Line too long (132/100) (line-too-long)
utils\preprocess.py:399:0: C0301: Line too long (112/100) (line-too-long)
utils\preprocess.py:401:0: C0301: Line too long (118/100) (line-too-long)
utils\preprocess.py:429:0: C0301: Line too long (104/100) (line-too-long)
utils\preprocess.py:433:0: C0301: Line too long (107/100) (line-too-long)
utils\preprocess.py:434:0: C0301: Line too long (108/100) (line-too-long)
utils\preprocess.py:464:0: C0301: Line too long (102/100) (line-too-long)
utils\preprocess.py:466:0: C0304: Final newline missing (missing-final-newline)
utils\preprocess.py:1:0: C0114: Missing module docstring (missing-module-docstring)
utils\preprocess.py:3:0: E0401: Unable to import 'utils.chords' (import-error)
utils\preprocess.py:6:0: E0401: Unable to import 'pyrubberband' (import-error)
utils\preprocess.py:10:0: C0115: Missing class docstring (missing-class-docstring)
utils\preprocess.py:11:4: C0103: Class constant name "cqt" doesn't conform to UPPER_CASE naming style (invalid-name)
utils\preprocess.py:13:0: C0115: Missing class docstring (missing-class-docstring)
utils\preprocess.py:21:8: C0103: Attribute name "Chord_class" doesn't conform to snake_case naming style (invalid-name)
utils\preprocess.py:13:0: R0902: Too many instance attributes (16/7) (too-many-instance-attributes)
utils\preprocess.py:40:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\preprocess.py:40:4: R1710: Either all return statements in a function should return an expression, or none of them should. (inconsistent-return-statements)
utils\preprocess.py:51:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\preprocess.py:51:4: R1710: Either all return statements in a function should return an expression, or none of them should. (inconsistent-return-statements)
utils\preprocess.py:62:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\preprocess.py:62:4: R0914: Too many local variables (18/15) (too-many-locals)
utils\preprocess.py:79:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
utils\preprocess.py:62:4: R0912: Too many branches (15/12) (too-many-branches)
utils\preprocess.py:116:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\preprocess.py:123:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\preprocess.py:131:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\preprocess.py:134:21: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\preprocess.py:137:25: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\preprocess.py:142:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\preprocess.py:142:4: R0914: Too many local variables (51/15) (too-many-locals)
utils\preprocess.py:197:20: R1723: Unnecessary "elif" after "break", remove the leading "el" from "elif" (no-else-break)
utils\preprocess.py:213:24: C0103: Variable name "curSec" doesn't conform to snake_case naming style (invalid-name)
utils\preprocess.py:229:102: W0640: Cell variable curSec defined in loop (cell-var-from-loop)
utils\preprocess.py:233:65: W0640: Cell variable curSec defined in loop (cell-var-from-loop)
utils\preprocess.py:244:32: W0707: Consider explicitly re-raising using 'raise RuntimeError() from e' (raise-missing-from)
utils\preprocess.py:252:32: C0103: Variable name "curSec" doesn't conform to snake_case naming style (invalid-name)
utils\preprocess.py:155:8: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
utils\preprocess.py:264:38: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\preprocess.py:266:38: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\preprocess.py:299:32: W0707: Consider explicitly re-raising using 'raise RuntimeError() from e' (raise-missing-from)
utils\preprocess.py:303:19: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\preprocess.py:142:4: R0912: Too many branches (25/12) (too-many-branches)
utils\preprocess.py:142:4: R0915: Too many statements (106/50) (too-many-statements)
utils\preprocess.py:155:8: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
utils\preprocess.py:305:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\preprocess.py:305:4: R0914: Too many local variables (51/15) (too-many-locals)
utils\preprocess.py:348:27: W0718: Catching too general exception Exception (broad-exception-caught)
utils\preprocess.py:350:35: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\preprocess.py:366:20: R1723: Unnecessary "elif" after "break", remove the leading "el" from "elif" (no-else-break)
utils\preprocess.py:382:24: C0103: Variable name "curSec" doesn't conform to snake_case naming style (invalid-name)
utils\preprocess.py:395:102: W0640: Cell variable curSec defined in loop (cell-var-from-loop)
utils\preprocess.py:397:95: W0640: Cell variable curSec defined in loop (cell-var-from-loop)
utils\preprocess.py:408:32: W0707: Consider explicitly re-raising using 'raise RuntimeError() from e' (raise-missing-from)
utils\preprocess.py:411:35: R1714: Consider merging these comparisons with 'in' by using 'chord not in (169, 168)'. Use a set instead if elements are hashable. (consider-using-in)
utils\preprocess.py:416:32: C0103: Variable name "curSec" doesn't conform to snake_case naming style (invalid-name)
utils\preprocess.py:317:8: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
utils\preprocess.py:428:38: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\preprocess.py:430:38: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\preprocess.py:462:32: W0707: Consider explicitly re-raising using 'raise RuntimeError() from e' (raise-missing-from)
utils\preprocess.py:466:19: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\preprocess.py:305:4: R0912: Too many branches (26/12) (too-many-branches)
utils\preprocess.py:305:4: R0915: Too many statements (114/50) (too-many-statements)
utils\preprocess.py:317:8: R1702: Too many nested blocks (7/5) (too-many-nested-blocks)
utils\preprocess.py:4:0: C0411: standard import "re" should be placed before third party import "librosa" and first party import "utils.chords.Chords"  (wrong-import-order)
utils\preprocess.py:5:0: C0411: standard import "enum.Enum" should be placed before third party import "librosa" and first party import "utils.chords.Chords"  (wrong-import-order)
utils\preprocess.py:6:0: C0411: third party import "pyrubberband" should be placed before first party import "utils.chords.Chords"  (wrong-import-order)
utils\preprocess.py:7:0: C0411: third party import "torch" should be placed before first party import "utils.chords.Chords"  (wrong-import-order)
utils\preprocess.py:8:0: C0411: standard import "math" should be placed before third party imports "librosa", "pyrubberband", "torch" and first party import "utils.chords.Chords"  (wrong-import-order)
************* Module Music2Emotion.utils.pytorch_utils
utils\pytorch_utils.py:23:0: C0301: Line too long (141/100) (line-too-long)
utils\pytorch_utils.py:24:0: C0301: Line too long (107/100) (line-too-long)
utils\pytorch_utils.py:1:0: C0114: Missing module docstring (missing-module-docstring)
utils\pytorch_utils.py:6:0: E0401: Unable to import 'utils' (import-error)
utils\pytorch_utils.py:13:0: C0116: Missing function or method docstring (missing-function-docstring)
utils\pytorch_utils.py:18:20: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\pytorch_utils.py:14:8: W0612: Unused variable 'i' (unused-variable)
utils\pytorch_utils.py:22:0: C0116: Missing function or method docstring (missing-function-docstring)
utils\pytorch_utils.py:23:7: E1123: Unexpected keyword argument 'map_location' in function call (unexpected-keyword-arg)
utils\pytorch_utils.py:23:56: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\pytorch_utils.py:24:66: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\pytorch_utils.py:28:20: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\pytorch_utils.py:30:20: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\pytorch_utils.py:4:0: C0411: standard import "os" should be placed before third party imports "torch", "numpy" (wrong-import-order)
utils\pytorch_utils.py:5:0: C0411: standard import "math" should be placed before third party imports "torch", "numpy" (wrong-import-order)
utils\pytorch_utils.py:3:0: W0611: Unused numpy imported as np (unused-import)
utils\pytorch_utils.py:5:0: W0611: Unused import math (unused-import)
************* Module Music2Emotion.utils.tf_logger
utils\tf_logger.py:70:0: C0304: Final newline missing (missing-final-newline)
utils\tf_logger.py:1:0: C0114: Missing module docstring (missing-module-docstring)
utils\tf_logger.py:11:0: C0115: Missing class docstring (missing-class-docstring)
utils\tf_logger.py:11:0: C0103: Class name "TF_Logger" doesn't conform to PascalCase naming style (invalid-name)
utils\tf_logger.py:11:0: R0205: Class 'TF_Logger' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)
utils\tf_logger.py:14:22: E1101: Module 'tensorboard.summary._tf.summary' has no 'FileWriter' member (no-member)
utils\tf_logger.py:14:22: E1101: Module 'tensorflow._api.v2.summary' has no 'FileWriter' member (no-member)
utils\tf_logger.py:18:18: E1101: Module 'tensorflow' has no 'Summary' member; maybe 'summary'? (no-member)
utils\tf_logger.py:18:36: E1101: Module 'tensorflow' has no 'Summary' member; maybe 'summary'? (no-member)
utils\tf_logger.py:29:12: W0702: No exception type(s) specified (bare-except)
utils\tf_logger.py:30:20: E0601: Using variable 'BytesIO' before assignment (used-before-assignment)
utils\tf_logger.py:31:12: E1101: Module 'scipy.misc' has no 'toimage' member (no-member)
utils\tf_logger.py:34:22: E1101: Module 'tensorflow' has no 'Summary' member; maybe 'summary'? (no-member)
utils\tf_logger.py:38:33: E1101: Module 'tensorflow' has no 'Summary' member; maybe 'summary'? (no-member)
utils\tf_logger.py:38:54: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\tf_logger.py:41:18: E1101: Module 'tensorflow' has no 'Summary' member; maybe 'summary'? (no-member)
utils\tf_logger.py:51:15: E1101: Module 'tensorflow' has no 'HistogramProto' member (no-member)
utils\tf_logger.py:68:18: E1101: Module 'tensorflow' has no 'Summary' member; maybe 'summary'? (no-member)
utils\tf_logger.py:68:36: E1101: Module 'tensorflow' has no 'Summary' member; maybe 'summary'? (no-member)
************* Module Music2Emotion.utils.transformer_modules
utils\transformer_modules.py:64:0: C0301: Line too long (121/100) (line-too-long)
utils\transformer_modules.py:144:0: C0301: Line too long (105/100) (line-too-long)
utils\transformer_modules.py:157:0: C0301: Line too long (101/100) (line-too-long)
utils\transformer_modules.py:179:0: C0301: Line too long (101/100) (line-too-long)
utils\transformer_modules.py:219:0: C0301: Line too long (108/100) (line-too-long)
utils\transformer_modules.py:235:0: C0301: Line too long (111/100) (line-too-long)
utils\transformer_modules.py:274:0: C0304: Final newline missing (missing-final-newline)
utils\transformer_modules.py:1:0: C0114: Missing module docstring (missing-module-docstring)
utils\transformer_modules.py:5:0: R0402: Use 'from torch import nn' instead (consider-using-from-import)
utils\transformer_modules.py:40:0: C0115: Missing class docstring (missing-class-docstring)
utils\transformer_modules.py:44:8: R1725: Consider using Python 3 style super() without arguments (super-with-arguments)
utils\transformer_modules.py:49:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\transformer_modules.py:60:8: R1725: Consider using Python 3 style super() without arguments (super-with-arguments)
utils\transformer_modules.py:67:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\transformer_modules.py:68:34: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\transformer_modules.py:74:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\transformer_modules.py:78:8: W0612: Unused variable 'topk' (unused-variable)
utils\transformer_modules.py:91:0: R0902: Too many instance attributes (9/7) (too-many-instance-attributes)
utils\transformer_modules.py:97:4: R0913: Too many arguments (9/5) (too-many-arguments)
utils\transformer_modules.py:97:4: R0917: Too many positional arguments (9/5) (too-many-positional-arguments)
utils\transformer_modules.py:109:8: R1725: Consider using Python 3 style super() without arguments (super-with-arguments)
utils\transformer_modules.py:113:29: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\transformer_modules.py:116:29: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\transformer_modules.py:159:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\transformer_modules.py:218:8: R1725: Consider using Python 3 style super() without arguments (super-with-arguments)
utils\transformer_modules.py:223:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\transformer_modules.py:235:4: R0913: Too many arguments (7/5) (too-many-arguments)
utils\transformer_modules.py:235:4: R0917: Too many positional arguments (7/5) (too-many-positional-arguments)
utils\transformer_modules.py:247:8: R1725: Consider using Python 3 style super() without arguments (super-with-arguments)
utils\transformer_modules.py:260:33: C0209: Formatting a regular string which could be an f-string (consider-using-f-string)
utils\transformer_modules.py:266:4: C0116: Missing function or method docstring (missing-function-docstring)
utils\transformer_modules.py:8:0: C0411: standard import "math" should be placed before third party imports "torch", "torch.nn", "torch.nn.functional", "numpy" (wrong-import-order)
************* Module Music2Emotion.utils.__init__
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.dataset_loaders.emomusic:[44:233]
==Music2Emotion.dataset_loaders.pmemo:[43:226]
            self.idx_to_chord = json.load(f)
            self.idx_to_chord = {int(k): v for k, v in self.idx_to_chord.items()}  # Ensure keys are ints

        with open('dataset/emomusic/meta/chord_root.json') as json_file:
            self.chordRootDic = json.load(json_file)
        with open('dataset/emomusic/meta/chord_attr.json') as json_file:
            self.chordAttrDic = json.load(json_file)


        # MERT and MP3 directories
        self.mert_dir = os.path.join(self.root, 'mert_30s')
        self.mp3_dir = os.path.join(self.root, 'mp3')

        # Load static annotations (valence and arousal)
        self.annotation_file = os.path.join(self.root, 'meta', 'static_annotations.csv')
        self.annotations = pd.read_csv(self.annotation_file, index_col='song_id')

        # Load static annotations (valence and arousal)
        self.annotation_tag_file = os.path.join(self.root, 'meta', 'mood_probabilities.csv')
        self.annotations_tag = pd.read_csv(self.annotation_tag_file, index_col='song_id')


    def __len__(self):
        return len(self.file_ids)

    def __getitem__(self, index):
        file_id = int(self.file_ids[index])  # File ID from split

        # Get valence and arousal from annotations
        if file_id not in self.annotations.index:
            raise ValueError(f"File ID {file_id} not found in annotations.")

        valence = self.annotations.loc[file_id, 'valence_mean']
        arousal = self.annotations.loc[file_id, 'arousal_mean']

        y_valence = torch.tensor(valence, dtype=torch.float32)
        y_arousal = torch.tensor(arousal, dtype=torch.float32)

        y_mood = np.array(self.annotations_tag.loc[file_id])
        y_mood = y_mood.astype('float32')
        y_mood = torch.from_numpy(y_mood)


        # --- Chord feature ---
        fn_chord = os.path.join(self.root, 'chord', 'lab3', str(file_id) + ".lab")

        chords = []

        if not os.path.exists(fn_chord):
            chords.append((float(0), float(0), "N"))
        else:
            with open(fn_chord, 'r') as file:
                for line in file:
                    start, end, chord = line.strip().split()
                    chords.append((float(start), float(end), chord))

        encoded = []
        encoded_root= []
        encoded_attr=[]
        durations = []
        for start, end, chord in chords:
            chord_arr = chord.split(":")
            if len(chord_arr) == 1:
                chordRootID = self.chordRootDic[chord_arr[0]]
                if chord_arr[0] == "N" or chord_arr[0] == "X":
                    chordAttrID = 0
                else:
                    chordAttrID = 1
            elif len(chord_arr) == 2:
                chordRootID = self.chordRootDic[chord_arr[0]]
                chordAttrID = self.chordAttrDic[chord_arr[1]]
            encoded_root.append(chordRootID)
            encoded_attr.append(chordAttrID)

            if chord in self.chord_to_idx:
                encoded.append(self.chord_to_idx[chord])
            else:
                print(f"Warning: Chord {chord} not found in chord.json. Skipping.")

            durations.append(end - start)  # Compute duration

        encoded_chords = np.array(encoded)
        encoded_chords_root = np.array(encoded_root)
        encoded_chords_attr = np.array(encoded_attr)

        # Maximum sequence length for chords
        max_sequence_length = 100  # Define this globally or as a parameter

        # Truncate or pad chord sequences
        if len(encoded_chords) > max_sequence_length:
            # Truncate to max length
            encoded_chords = encoded_chords[:max_sequence_length]
            encoded_chords_root = encoded_chords_root[:max_sequence_length]
            encoded_chords_attr = encoded_chords_attr[:max_sequence_length]

        else:
            # Pad with zeros (padding value for chords)
            padding = [0] * (max_sequence_length - len(encoded_chords))
            encoded_chords = np.concatenate([encoded_chords, padding])
            encoded_chords_root = np.concatenate([encoded_chords_root, padding])
            encoded_chords_attr = np.concatenate([encoded_chords_attr, padding])

        # Convert to tensor
        chords_tensor = torch.tensor(encoded_chords, dtype=torch.long)  # Fixed length tensor
        chords_root_tensor = torch.tensor(encoded_chords_root, dtype=torch.long)  # Fixed length tensor
        chords_attr_tensor = torch.tensor(encoded_chords_attr, dtype=torch.long)  # Fixed length tensor

        # --- Key feature (Tonic and Mode separation) ---
        fn_key = os.path.join(self.root, 'key', str(file_id) + ".lab")

        if not os.path.exists(fn_key):
            mode = "major"
        else:
            mode = "major"  # Default value
            with open(fn_key, 'r') as file:
                for line in file:
                    key = line.strip()
            if key == "None":
                mode = "major"
            else:
                mode = key.split()[-1]

        encoded_mode = self.mode_to_idx.get(mode, 0)
        mode_tensor = torch.tensor([encoded_mode], dtype=torch.long)

        # --- MERT feature ---
        fn_mert = os.path.join(self.mert_dir, str(file_id))

        embeddings = []

        # Specify the layers to extract (3rd, 6th, 9th, and 12th layers)
        layers_to_extract = self.cfg.model.layers

        # Collect all segment embeddings
        segment_embeddings = []
        for filename in sorted(os.listdir(fn_mert)):  # Sort files to ensure sequential order
            file_path = os.path.join(fn_mert, filename)
            if os.path.isfile(file_path) and filename.endswith('.npy'):
                segment = np.load(file_path)

                # Extract and concatenate features for the specified layers
                concatenated_features = np.concatenate(
                    [segment[:, layer_idx, :] for layer_idx in layers_to_extract], axis=1
                )
                concatenated_features = np.squeeze(concatenated_features)  # Shape: 768 * 2 = 1536
                segment_embeddings.append(concatenated_features)

        # Convert to numpy array
        segment_embeddings = np.array(segment_embeddings)

        # Check mode: 'train' or 'val'
        if self.tr_val == "train" and len(segment_embeddings) > 0:  # Augmentation for training
            num_segments = len(segment_embeddings)

            # Randomly choose a starting index and the length of the sequence
            start_idx = np.random.randint(0, num_segments)  # Random starting index
            end_idx = np.random.randint(start_idx + 1, num_segments + 1)  # Ensure end index is after start index

            # Extract the sequential subset
            chosen_segments = segment_embeddings[start_idx:end_idx]

            # Compute the mean of the chosen sequential segments
            final_embedding_mert = np.mean(chosen_segments, axis=0)
        else:  # Validation or other modes: Use mean of all segments
            if len(segment_embeddings) > 0:
                final_embedding_mert = np.mean(segment_embeddings, axis=0)
            else:
                # Handle case with no valid embeddings
                final_embedding_mert = np.zeros((1536,))  # Example: Return zero vector of appropriate size

        # Convert to PyTorch tensor
        final_embedding_mert = torch.from_numpy(final_embedding_mert)


        # Get the MP3 path
        mp3_path = os.path.join(self.mp3_dir, f"{file_id}.mp3")
        if not os.path.exists(mp3_path):
            raise FileNotFoundError(f"MP3 file not found for {mp3_path}")

        return {
            "x_mert": final_embedding_mert,
            "x_chord" : chords_tensor,
            "x_chord_root" : chords_root_tensor,
            "x_chord_attr" : chords_attr_tensor,
            "x_key" : mode_tensor,
            "y_va": torch.stack([y_valence, y_arousal], dim=0),
            "y_mood" : y_mood,
            "path": mp3_path
        } (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.dataset_loaders.deam:[68:235]
==Music2Emotion.dataset_loaders.emomusic:[67:233]
        return len(self.file_ids)

    def __getitem__(self, index):
        file_id = int(self.file_ids[index])  # File ID from split

        # Get valence and arousal from annotations
        if file_id not in self.annotations.index:
            raise ValueError(f"File ID {file_id} not found in annotations.")

        valence = self.annotations.loc[file_id, 'valence_mean']
        arousal = self.annotations.loc[file_id, 'arousal_mean']

        y_valence = torch.tensor(valence, dtype=torch.float32)
        y_arousal = torch.tensor(arousal, dtype=torch.float32)

        y_mood = np.array(self.annotations_tag.loc[file_id])
        y_mood = y_mood.astype('float32')
        y_mood = torch.from_numpy(y_mood)


        # --- Chord feature ---
        fn_chord = os.path.join(self.root, 'chord', 'lab3', str(file_id) + ".lab")

        chords = []

        if not os.path.exists(fn_chord):
            chords.append((float(0), float(0), "N"))
        else:
            with open(fn_chord, 'r') as file:
                for line in file:
                    start, end, chord = line.strip().split()
                    chords.append((float(start), float(end), chord))

        encoded = []
        encoded_root= []
        encoded_attr=[]
        durations = []
        for start, end, chord in chords:
            chord_arr = chord.split(":")
            if len(chord_arr) == 1:
                chordRootID = self.chordRootDic[chord_arr[0]]
                if chord_arr[0] == "N" or chord_arr[0] == "X":
                    chordAttrID = 0
                else:
                    chordAttrID = 1
            elif len(chord_arr) == 2:
                chordRootID = self.chordRootDic[chord_arr[0]]
                chordAttrID = self.chordAttrDic[chord_arr[1]]
            encoded_root.append(chordRootID)
            encoded_attr.append(chordAttrID)

            if chord in self.chord_to_idx:
                encoded.append(self.chord_to_idx[chord])
            else:
                print(f"Warning: Chord {chord} not found in chord.json. Skipping.")

            durations.append(end - start)  # Compute duration

        encoded_chords = np.array(encoded)
        encoded_chords_root = np.array(encoded_root)
        encoded_chords_attr = np.array(encoded_attr)

        # Maximum sequence length for chords
        max_sequence_length = 100  # Define this globally or as a parameter

        # Truncate or pad chord sequences
        if len(encoded_chords) > max_sequence_length:
            # Truncate to max length
            encoded_chords = encoded_chords[:max_sequence_length]
            encoded_chords_root = encoded_chords_root[:max_sequence_length]
            encoded_chords_attr = encoded_chords_attr[:max_sequence_length]

        else:
            # Pad with zeros (padding value for chords)
            padding = [0] * (max_sequence_length - len(encoded_chords))
            encoded_chords = np.concatenate([encoded_chords, padding])
            encoded_chords_root = np.concatenate([encoded_chords_root, padding])
            encoded_chords_attr = np.concatenate([encoded_chords_attr, padding])

        # Convert to tensor
        chords_tensor = torch.tensor(encoded_chords, dtype=torch.long)  # Fixed length tensor
        chords_root_tensor = torch.tensor(encoded_chords_root, dtype=torch.long)  # Fixed length tensor
        chords_attr_tensor = torch.tensor(encoded_chords_attr, dtype=torch.long)  # Fixed length tensor

        # --- Key feature (Tonic and Mode separation) ---
        fn_key = os.path.join(self.root, 'key', str(file_id) + ".lab")

        if not os.path.exists(fn_key):
            mode = "major"
        else:
            mode = "major"  # Default value
            with open(fn_key, 'r') as file:
                for line in file:
                    key = line.strip()
            if key == "None":
                mode = "major"
            else:
                mode = key.split()[-1]

        encoded_mode = self.mode_to_idx.get(mode, 0)
        mode_tensor = torch.tensor([encoded_mode], dtype=torch.long)

        # --- MERT feature ---
        fn_mert = os.path.join(self.mert_dir, str(file_id))

        embeddings = []

        # Specify the layers to extract (3rd, 6th, 9th, and 12th layers)
        layers_to_extract = self.cfg.model.layers

        # Collect all segment embeddings
        segment_embeddings = []
        for filename in sorted(os.listdir(fn_mert)):  # Sort files to ensure sequential order
            file_path = os.path.join(fn_mert, filename)
            if os.path.isfile(file_path) and filename.endswith('.npy'):
                segment = np.load(file_path)

                # Extract and concatenate features for the specified layers
                concatenated_features = np.concatenate(
                    [segment[:, layer_idx, :] for layer_idx in layers_to_extract], axis=1
                )
                concatenated_features = np.squeeze(concatenated_features)  # Shape: 768 * 2 = 1536
                segment_embeddings.append(concatenated_features)

        # Convert to numpy array
        segment_embeddings = np.array(segment_embeddings)

        # Check mode: 'train' or 'val'
        if self.tr_val == "train" and len(segment_embeddings) > 0:  # Augmentation for training
            num_segments = len(segment_embeddings)

            # Randomly choose a starting index and the length of the sequence
            start_idx = np.random.randint(0, num_segments)  # Random starting index
            end_idx = np.random.randint(start_idx + 1, num_segments + 1)  # Ensure end index is after start index

            # Extract the sequential subset
            chosen_segments = segment_embeddings[start_idx:end_idx]

            # Compute the mean of the chosen sequential segments
            final_embedding_mert = np.mean(chosen_segments, axis=0)
        else:  # Validation or other modes: Use mean of all segments
            if len(segment_embeddings) > 0:
                final_embedding_mert = np.mean(segment_embeddings, axis=0)
            else:
                # Handle case with no valid embeddings
                final_embedding_mert = np.zeros((1536,))  # Example: Return zero vector of appropriate size

        # Convert to PyTorch tensor
        final_embedding_mert = torch.from_numpy(final_embedding_mert)


        # Get the MP3 path
        mp3_path = os.path.join(self.mp3_dir, f"{file_id}.mp3")
        if not os.path.exists(mp3_path):
            raise FileNotFoundError(f"MP3 file not found for {mp3_path}")

        return {
            "x_mert": final_embedding_mert,
            "x_chord" : chords_tensor,
            "x_chord_root" : chords_root_tensor,
            "x_chord_attr" : chords_attr_tensor,
            "x_key" : mode_tensor,
            "y_va": torch.stack([y_valence, y_arousal], dim=0),
            "y_mood" : y_mood,
            "path": mp3_path
        } (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.dataset_loaders.deam:[91:154]
==Music2Emotion.dataset_loaders.jamendo:[87:149]
        chords = []

        if not os.path.exists(fn_chord):
            chords.append((float(0), float(0), "N"))
        else:
            with open(fn_chord, 'r') as file:
                for line in file:
                    start, end, chord = line.strip().split()
                    chords.append((float(start), float(end), chord))

        encoded = []
        encoded_root= []
        encoded_attr=[]
        durations = []
        for start, end, chord in chords:
            chord_arr = chord.split(":")
            if len(chord_arr) == 1:
                chordRootID = self.chordRootDic[chord_arr[0]]
                if chord_arr[0] == "N" or chord_arr[0] == "X":
                    chordAttrID = 0
                else:
                    chordAttrID = 1
            elif len(chord_arr) == 2:
                chordRootID = self.chordRootDic[chord_arr[0]]
                chordAttrID = self.chordAttrDic[chord_arr[1]]
            encoded_root.append(chordRootID)
            encoded_attr.append(chordAttrID)

            if chord in self.chord_to_idx:
                encoded.append(self.chord_to_idx[chord])
            else:
                print(f"Warning: Chord {chord} not found in chord.json. Skipping.")

            durations.append(end - start)  # Compute duration

        encoded_chords = np.array(encoded)
        encoded_chords_root = np.array(encoded_root)
        encoded_chords_attr = np.array(encoded_attr)

        # Maximum sequence length for chords
        max_sequence_length = 100  # Define this globally or as a parameter

        # Truncate or pad chord sequences
        if len(encoded_chords) > max_sequence_length:
            # Truncate to max length
            encoded_chords = encoded_chords[:max_sequence_length]
            encoded_chords_root = encoded_chords_root[:max_sequence_length]
            encoded_chords_attr = encoded_chords_attr[:max_sequence_length]

        else:
            # Pad with zeros (padding value for chords)
            padding = [0] * (max_sequence_length - len(encoded_chords))
            encoded_chords = np.concatenate([encoded_chords, padding])
            encoded_chords_root = np.concatenate([encoded_chords_root, padding])
            encoded_chords_attr = np.concatenate([encoded_chords_attr, padding])

        # Convert to tensor
        chords_tensor = torch.tensor(encoded_chords, dtype=torch.long)  # Fixed length tensor
        chords_root_tensor = torch.tensor(encoded_chords_root, dtype=torch.long)  # Fixed length tensor
        chords_attr_tensor = torch.tensor(encoded_chords_attr, dtype=torch.long)  # Fixed length tensor

        # --- Key feature (Tonic and Mode separation) --- (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.dataset_loaders.deam:[175:221]
==Music2Emotion.dataset_loaders.jamendo:[168:214]
        embeddings = []

        # Specify the layers to extract (3rd, 6th, 9th, and 12th layers)
        layers_to_extract = self.cfg.model.layers

        # Collect all segment embeddings
        segment_embeddings = []
        for filename in sorted(os.listdir(fn_mert)):  # Sort files to ensure sequential order
            file_path = os.path.join(fn_mert, filename)
            if os.path.isfile(file_path) and filename.endswith('.npy'):
                segment = np.load(file_path)

                # Extract and concatenate features for the specified layers
                concatenated_features = np.concatenate(
                    [segment[:, layer_idx, :] for layer_idx in layers_to_extract], axis=1
                )
                concatenated_features = np.squeeze(concatenated_features)  # Shape: 768 * 2 = 1536
                segment_embeddings.append(concatenated_features)

        # Convert to numpy array
        segment_embeddings = np.array(segment_embeddings)

        # Check mode: 'train' or 'val'
        if self.tr_val == "train" and len(segment_embeddings) > 0:  # Augmentation for training
            num_segments = len(segment_embeddings)

            # Randomly choose a starting index and the length of the sequence
            start_idx = np.random.randint(0, num_segments)  # Random starting index
            end_idx = np.random.randint(start_idx + 1, num_segments + 1)  # Ensure end index is after start index

            # Extract the sequential subset
            chosen_segments = segment_embeddings[start_idx:end_idx]

            # Compute the mean of the chosen sequential segments
            final_embedding_mert = np.mean(chosen_segments, axis=0)
        else:  # Validation or other modes: Use mean of all segments
            if len(segment_embeddings) > 0:
                final_embedding_mert = np.mean(segment_embeddings, axis=0)
            else:
                # Handle case with no valid embeddings
                final_embedding_mert = np.zeros((1536,))  # Example: Return zero vector of appropriate size

        # Convert to PyTorch tensor
        final_embedding_mert = torch.from_numpy(final_embedding_mert)

        # Get the MP3 path (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.utils.chords:[522:541]
==Music2Emotion.utils.mir_eval_modules:[184:205]
        for i in range(len(ref_labels)):
            if ref_labels[i][-2:] == ':4':
                ref_labels[i] = ref_labels[i].replace(':4', ':sus4')
            elif ref_labels[i][-2:] == ':6':
                ref_labels[i] = ref_labels[i].replace(':6', ':maj6')
            elif ref_labels[i][-4:] == ':6/2':
                ref_labels[i] = ref_labels[i].replace(':6/2', ':maj6/2')
            elif ref_labels[i] == 'Emin/4':
                ref_labels[i] = 'E:min/4'
            elif ref_labels[i] == 'A7/3':
                ref_labels[i] = 'A:7/3'
            elif ref_labels[i] == 'Bb7/3':
                ref_labels[i] = 'Bb:7/3'
            elif ref_labels[i] == 'Bb7/5':
                ref_labels[i] = 'Bb:7/5'
            elif ref_labels[i].find(':') == -1:
                if ref_labels[i].find('min') != -1:
                    ref_labels[i] = ref_labels[i][:ref_labels[i].find('min')] + ':' + ref_labels[i][ref_labels[i].find('min'):]
        return ref_labels (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.model.linear_attn_ck:[57:90]
==Music2Emotion.model.linear_mt_attn_ck:[64:96]
        )


    def forward(self, model_input_dic ):
        x_mert = model_input_dic["x_mert"]
        x_chord_root = model_input_dic["x_chord_root"]
        x_chord_attr = model_input_dic["x_chord_attr"]

        x_key = model_input_dic["x_key"]
        key_embedding = x_key.float()

        chord_root_embedding = self.chord_root_embedding(x_chord_root)  # Shape: (batch_size, seq_len, chord_root_emb_dim)
        chord_attr_embedding = self.chord_attr_embedding(x_chord_attr)  # Shape: (batch_size, seq_len, chord_attr_emb_dim)

        # Concatenate root and attribute embeddings
        chord_combined_embedding = torch.cat(
            (chord_root_embedding, chord_attr_embedding), dim=-1
        )  # Shape: (batch_size, seq_len, chord_root_emb_dim + chord_attr_emb_dim)

        chord_combined_embedding = self.positional_encoding(chord_combined_embedding)
        cls_token = torch.zeros_like(chord_combined_embedding[:, :1, :])

        chord_embedding_with_cls = torch.cat([cls_token, chord_combined_embedding], dim=1)  # Add CLS at the start
        chord_embedding_transformed = self.chord_transformer(chord_embedding_with_cls)  # Shape: (seq_len+1, batch_size, chord_emb_dim)

        chord_embedding_cls = chord_embedding_transformed[:,0,:]  # Shape: (batch_size, chord_emb_dim)

        # Combine all features
        combined_features = torch.cat((x_mert, chord_embedding_cls, key_embedding), dim=1)
        # Input projection
        combined_features = self.input_proj(combined_features)  # Shape: (batch_size, d_model)
 (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.model.linear_attn_ck:[26:53]
==Music2Emotion.model.linear_mt_attn_ck:[26:53]
        super().__init__()
        self.d_model = 512

        self.d_model_transformer = chord_root_emb_dim + chord_attr_emb_dim

        # Embedding layers for chords and keys
        self.chord_root_embedding = nn.Embedding(num_chords_root, chord_root_emb_dim)
        self.chord_attr_embedding = nn.Embedding(num_chords_attr, chord_attr_emb_dim)

        nn.init.xavier_uniform_(self.chord_root_embedding.weight)
        nn.init.xavier_uniform_(self.chord_attr_embedding.weight)

        # Positional encoding for chord progression
        self.positional_encoding = PositionalEncoding(self.d_model_transformer)

        # Transformer for chord progression modeling
        self.chord_transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=self.d_model_transformer, nhead=nhead, dim_feedforward= 64, dropout=0.1, batch_first=True),
            num_layers=2
        )
        # Input projection for latent features
        self.input_proj = nn.Sequential(
            nn.Linear(input_size +  self.d_model_transformer + 1, self.d_model),
            nn.ReLU(),
        )

        # Output projection (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.dataset_loaders.deam:[121:148]
==Music2Emotion.music2emo:[428:455]
            else:
                print(f"Warning: Chord {chord} not found in chord.json. Skipping.")

            durations.append(end - start)  # Compute duration

        encoded_chords = np.array(encoded)
        encoded_chords_root = np.array(encoded_root)
        encoded_chords_attr = np.array(encoded_attr)

        # Maximum sequence length for chords
        max_sequence_length = 100  # Define this globally or as a parameter

        # Truncate or pad chord sequences
        if len(encoded_chords) > max_sequence_length:
            # Truncate to max length
            encoded_chords = encoded_chords[:max_sequence_length]
            encoded_chords_root = encoded_chords_root[:max_sequence_length]
            encoded_chords_attr = encoded_chords_attr[:max_sequence_length]

        else:
            # Pad with zeros (padding value for chords)
            padding = [0] * (max_sequence_length - len(encoded_chords))
            encoded_chords = np.concatenate([encoded_chords, padding])
            encoded_chords_root = np.concatenate([encoded_chords_root, padding])
            encoded_chords_attr = np.concatenate([encoded_chords_attr, padding])

        # Convert to tensor (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.dataset_loaders.deam:[156:173]
==Music2Emotion.dataset_loaders.jamendo:[151:167]
        if not os.path.exists(fn_key):
            mode = "major"
        else:
            mode = "major"  # Default value
            with open(fn_key, 'r') as file:
                for line in file:
                    key = line.strip()
            if key == "None":
                mode = "major"
            else:
                mode = key.split()[-1]

        encoded_mode = self.mode_to_idx.get(mode, 0)
        mode_tensor = torch.tensor([encoded_mode], dtype=torch.long)


        # --- MERT feature --- (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.dataset_loaders.emomusic:[20:41]
==Music2Emotion.dataset_loaders.pmemo:[20:40]
        self.segment_type = task_args.get('segment_type', "all")
        self.cfg = task_args.get('cfg')

        # Path to the split file (train/val/test)
        self.split_file = os.path.join(self.root, 'meta', 'split', f"{self.tr_val}.txt")

        # Read file IDs from the split file
        with open(self.split_file, 'r') as f:
            self.file_ids = [line.strip() for line in f.readlines()]

        # Separate tonic and mode
        tonic_signatures = ["A", "A#", "B", "C", "C#", "D", "D#", "E", "F", "F#", "G", "G#"]
        mode_signatures = ["major", "minor"]  # Major and minor modes

        self.tonic_to_idx = {tonic: idx for idx, tonic in enumerate(tonic_signatures)}
        self.mode_to_idx = {mode: idx for idx, mode in enumerate(mode_signatures)}

        self.idx_to_tonic = {idx: tonic for tonic, idx in self.tonic_to_idx.items()}
        self.idx_to_mode = {idx: mode for mode, idx in self.mode_to_idx.items()}

 (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.model.linear_attn_ck:[8:22]
==Music2Emotion.model.linear_mt_attn_ck:[8:22]
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=100):
        super().__init__()
        self.encoding = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        self.encoding[:, 0::2] = torch.sin(position * div_term)
        self.encoding[:, 1::2] = torch.cos(position * div_term)
        self.encoding = self.encoding.unsqueeze(0)  # Shape: (1, max_len, d_model)

    def forward(self, x):
        seq_len = x.size(1)
        return x + self.encoding[:, :seq_len, :].to(x.device)
 (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.dataset_loaders.deam:[97:108]
==Music2Emotion.music2emo:[403:415]
                for line in file:
                    start, end, chord = line.strip().split()
                    chords.append((float(start), float(end), chord))

        encoded = []
        encoded_root= []
        encoded_attr=[]
        durations = []
        for start, end, chord in chords:
            chord_arr = chord.split(":")
            if len(chord_arr) == 1: (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.dataset_loaders.deam:[184:198]
==Music2Emotion.music2emo:[242:251]
            if os.path.isfile(file_path) and filename.endswith('.npy'):
                segment = np.load(file_path)
                concatenated_features = np.concatenate(
                    [segment[:, layer_idx, :] for layer_idx in layers_to_extract], axis=1
                )
                concatenated_features = np.squeeze(concatenated_features)  # Shape: 768 * 2 = 1536
                segment_embeddings.append(concatenated_features)

        segment_embeddings = np.array(segment_embeddings) (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.music2emo:[288:296]
==Music2Emotion.utils.mir_eval_modules:[221:229]
        num_pad = n_timestep - (feature.shape[0] % n_timestep)
        feature = np.pad(feature, ((0, num_pad), (0, 0)), mode="constant", constant_values=0)
        num_instance = feature.shape[0] // n_timestep

        start_time = 0.0
        lines = []
        with torch.no_grad():
            model.eval() (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.dataset_loaders.jamendo:[53:62]
==Music2Emotion.dataset_loaders.pmemo:[43:51]
            self.idx_to_chord = json.load(f)
            self.idx_to_chord = {int(k): v for k, v in self.idx_to_chord.items()}  # Ensure keys are ints
        with open('dataset/emomusic/meta/chord_root.json') as json_file:
            self.chordRootDic = json.load(json_file)
        with open('dataset/emomusic/meta/chord_attr.json') as json_file:
            self.chordAttrDic = json.load(json_file)

        # MERT and MP3 directories (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.dataset_loaders.jamendo:[33:43]
==Music2Emotion.dataset_loaders.pmemo:[31:40]
        tonic_signatures = ["A", "A#", "B", "C", "C#", "D", "D#", "E", "F", "F#", "G", "G#"]
        mode_signatures = ["major", "minor"]  # Major and minor modes

        self.tonic_to_idx = {tonic: idx for idx, tonic in enumerate(tonic_signatures)}
        self.mode_to_idx = {mode: idx for idx, mode in enumerate(mode_signatures)}

        self.idx_to_tonic = {idx: tonic for tonic, idx in self.tonic_to_idx.items()}
        self.idx_to_mode = {idx: mode for mode, idx in self.mode_to_idx.items()}

        # Load the CSV file (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.dataset_loaders.deam:[57:68]
==Music2Emotion.dataset_loaders.emomusic:[44:54]
            self.idx_to_chord = json.load(f)
            self.idx_to_chord = {int(k): v for k, v in self.idx_to_chord.items()}  # Ensure keys are ints

        with open('dataset/emomusic/meta/chord_root.json') as json_file:
            self.chordRootDic = json.load(json_file)
        with open('dataset/emomusic/meta/chord_attr.json') as json_file:
            self.chordAttrDic = json.load(json_file)


        # MERT and MP3 directories (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.dataset_loaders.deam:[35:46]
==Music2Emotion.dataset_loaders.emomusic:[31:41]
        tonic_signatures = ["A", "A#", "B", "C", "C#", "D", "D#", "E", "F", "F#", "G", "G#"]
        mode_signatures = ["major", "minor"]  # Major and minor modes

        self.tonic_to_idx = {tonic: idx for idx, tonic in enumerate(tonic_signatures)}
        self.mode_to_idx = {mode: idx for idx, mode in enumerate(mode_signatures)}

        self.idx_to_tonic = {idx: tonic for tonic, idx in self.tonic_to_idx.items()}
        self.idx_to_mode = {idx: mode for mode, idx in self.mode_to_idx.items()}

 (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.music2emo:[301:307]
==Music2Emotion.utils.mir_eval_modules:[237:243]
                    for i in range(n_timestep):
                        if t == 0 and i == 0:
                            prev_chord = prediction[i].item()
                            continue
                        if prediction[i].item() != prev_chord:
                            lines.append( (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.dataset_loaders.deam:[20:31]
==Music2Emotion.dataset_loaders.emomusic:[20:31]
        self.segment_type = task_args.get('segment_type', "all")
        self.cfg = task_args.get('cfg')

        # Path to the split file (train/val/test)
        self.split_file = os.path.join(self.root, 'meta', 'split', f"{self.tr_val}.txt")

        # Read file IDs from the split file
        with open(self.split_file, 'r') as f:
            self.file_ids = [line.strip() for line in f.readlines()]

        # MERT and MP3 directories (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.dataset_loaders.deam:[211:221]
==Music2Emotion.music2emo:[251:257]
        if len(segment_embeddings) > 0:
            final_embedding_mert = np.mean(segment_embeddings, axis=0)
        else:
            final_embedding_mert = np.zeros((1536,))

        final_embedding_mert = torch.from_numpy(final_embedding_mert) (duplicate-code)
utils\__init__.py:1:0: R0801: Similar lines in 2 files
==Music2Emotion.dataset_loaders.deam:[109:114]
==Music2Emotion.music2emo:[416:421]
                if chord_arr[0] == "N" or chord_arr[0] == "X":
                    chordAttrID = 0
                else:
                    chordAttrID = 1
            elif len(chord_arr) == 2: (duplicate-code)

------------------------------------------------------------------
Your code has been rated at 5.11/10 (previous run: 5.11/10, +0.00)

